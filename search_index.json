[
["index.html", "Notes Notes 1 笔记概述 1.1 笔记目录 1.2 简明数据分析知识框架", " Notes Miao YU 2016-09-20 Notes 1 笔记概述 这里的笔记主要内容偏向数据分析与应用，其他主题的请移步这里 1.1 笔记目录 02-10 数据科学系列课程 11 统计学习导论 12 基因组学数据分析 1.2 简明数据分析知识框架 1.2.1 概率与分布 从可能性到独立事件概率计算 从联合概率到条件概率到贝叶斯公式 事件的发生空间到分布 多事件发生概率比较到标准化分布-z值 正态分布评价拟合 贝努利分布 二项分布，固定总数，成功概率，二项分布可用正态分布近似求值，也可用二项分布取精确值，求区间概率要扩大 负二项分布，固定成功次数概率 几何分布，最后一次成功概率 超几何分布，不放回抽样，成功概率 泊松分布，实验次数多，概率小，发生概率，泊松过程 1.2.2 统计量 总体到样本 多个事件的描述到众数 中位数 再到期望 描述多个事件的变动到方差 取样方法：随机，分层，分类 样本独立性:简单随机取样，样本数少于10%的总体可认为独立样本 估计的偏差为标准误 点估计到区间估计 标准误只针对样本均值，理解为样本均值的估计标准差 置信区间为对所有样本进行区间估计，95%的区间包含真值，是对总体参数的估计，近似认为样本符合某分布 中心极限法则：样本均值的分布为正态分布 1.2.3 统计推断 假设检验 不拒绝H0不代表H0是对的，拒绝H0代表HA可能正确，观察数值的区间重叠状况 使用双重否定进行描述 type I 假阳性 type II 假阴性 置信水平反映两种错误的可能性 p值描述某数值在H0（一般为等式）中出现的可能性，通常与置信水平对比，两边与单边 构建符合某分布的统计量进行参数估计，通过标准误计算p值，进行假设检验过程 功效表示HA拒绝H0的可能性，功效高，检验可靠 统计差异显著不代表实际差异显著，甚至没有实际意义 均值比较（连续） 配对数据 均值比较 t分布与自由度及小样本均值的标准误估计 置信区间与p值 样本均值的t检验 多组数据均值的方差分析与F检验 多重比较的假阳性问题 样本数足够可用统计模拟的方法进行检验，数据存在层级结构则不可直接模拟 比例比较（计数） 比例检验，计算基于H0的标准误，计算z值，计算p值，可反推样品量 比例差异检验，H0为比例相等，估计混合概率，计算标准误进行检验 记分检验与Wald检验 优度拟合 分布检验到卡方检验 独立性检验 精确检验 1.2.4 线性模型 变量关系到线性回归到线性诊断 参数估计到关系解释及误差分析 多元回归 模型选择 方差分析 非线性模型与平滑 logistic模型到广义线性模型 线性混合模型 主成分分析与因子分析 1.2.5 其他主题 非参数统计 贝叶斯统计 判别分析 岭回归与lasso 广义加性模型 鲁棒模型 决策树到随机森林 人工神经网络 支持向量机 蒙特卡洛分析到统计模拟 图论 1.2.6 应用 实验设计 模式识别 流行病学 生物信息学 化学信息学 心理学 空间数据分析 时间序列分析与信号处理 量化投资 "],
["section-2.html", "Notes 2 数据科学家工具箱 2.1 链接", " Notes 2 数据科学家工具箱 CLT name of root is represented by a slash: / home directory is represented by a tilde: ~ pwd print working directory recipe: command -flags arguments clear: clear out the commands in your current CLI window ls lists files and folders in the current directory -a lists hidden and unhidden files and folders -al lists details for hidden and unhidden files and folders cd stands for “change directory” cd takes as an argument the directory you want to visit cd with no argument takes you to your home directory cd .. allows you to chnage directory to one level above your current directory mkdir stands for “make directory” touch creates an empty file cp stands for “copy” cp takes as its first argument a file, and as its second argument the path to where you want the file to be copied cp can also be used for copying the contents of directories, but you must use the -r flag rm stands for “remove” use rm to delete entire directories and their contents by using the -r flag mv stands for “move” move files between directories use mv to rename files echo will print whatever arguments you provide date will print today’s date git $ git config --global user.name &quot;Your Name Here&quot; # 输入用户名 $ git config --global user.email &quot;your_email@example.com&quot; # 输入邮箱 $ git config --list # 检查 $ git init # 初始化目录 $ git add . # 添加新文件 $ git add -u # 更新改名或删除的文件 $ git add -A|git add --all # 添加所有改动 $ git commit -m &quot;your message goes here&quot; # 描述并缓存本地工作区改动到上一次commit $ git log # 查看commit记录 用Q退出 $ git status # 查看状态 $ git remote add # 添加服务器端地址 $ git remote -v # 查看远端状态 $ git push # 将本地commit推送到github服务器端 $ git pull|fetch|merge|clone # 本地获取远端repo $ exit # 退出 Git = Local (on your computer); GitHub = Remote (on the web) 基本问题 描述分析：对数据进行描述但不解释 探索分析：寻找未知的变量间关系 （相关不代表因果） 推断分析：用小样本推断总体 统计模型的目标 强依赖采样过程 预测分析：用一组变量预测另一变量 不一定有因果关系 因果分析：改变一个变量引发另一个变量变化的分析 随机实验 平均效果 机理分析：对个体改变一个变量所导致另一个变量的精确变化 公式模拟与参数拟合 数据次于问题 大数据依赖科学而不是数据 实验设计 重视可重复性随机与分组 预测与推断不同 不要选数据 2.1 链接 统计问题 R问题 R mailling ist 数据分享 "],
["r.html", "Notes 3 R语言编程 3.1 R语言概述 3.2 获得帮助 3.3 数据类型及基本运算 3.4 截取数据 3.5 读取数据 3.6 控制结构 3.7 函数 3.8 编程标准 3.9 范围规则 3.10 向量化操作 3.11 日期与时间 3.12 循环 3.13 模拟 3.14 调试 3.15 分析代码", " Notes 3 R语言编程 3.1 R语言概述 R语言是S语言的一种方言 1976年S是John Chambers等在贝尔实验室作为Fortran的扩展库开发出来的 1988年用C语言重写 S3方法 白皮书 1993年StatSci从贝尔实验室获得S语言的独家开发售卖许可 1998年S4方法 绿皮书 之后S语言稳定 获得Association for Computing Machinery’s Software System Award 2004年Insightful（原StatSci）从Lucent收购了S语言 2006年Alcatel收购了Lucent成立Alcatel-Lucent 2008年TIBCO收购Insightful 之前Insightful开发并售卖S-PLUS 1991年Ross Ihaka与Robert GentlemanNew在Zealand开发了R 1993年发布R第一份许可 1995年R作为自由软件发放GUN许可 1996年R邮件列表创立 1997年R Core成立 控制R源码 2000年R version 1.0.0 放出 2013年R version 3.0.2 放出 R由CRAN掌控的base包与其他包组成 其余参考R主页 3.2 获得帮助 help() ?command # 提问给出以下信息 version str(.Platform) 3.3 数据类型及基本运算 所有数据都是对象 所有对象都有类型 基本类型包括：字符“” 数字 整数L 复数(Re实部 Im虚部) 逻辑 向量储存同一类型数据 list存储不同类型数据 [[*]]引用相应向量 unlist 可用做紧凑输出 对象可以有属性attributes 对象赋值符号为 &lt;- 赋值同时展示加括号或直接输入对象名 可累加赋值 a &lt;- b &lt;- c #表示注释 不执行 : 用来产生整数序列 也可以用seq生成 向量用c产生 空向量用vector()函数建立 向量中类型不同的对象元素会被强制转换为同一类型 字符优先级最高 其次数字 其次逻辑(0 or 1) 也可以用来串联字符 可使用as.*来强制转化数据类型 对象可以用names命名 变量名开头不能是数字和. 大小写敏感 下划线不要出现在名字里 分割用. 变量名中不能有空格 保留字符 FALSE Inf NA NaN NULL TRUE break else for function if in next repeat while 清空rm(list = ls()) 矩阵 带有dimension属性的向量为矩阵 矩阵的生成次序为upper-left matrix(1:6,nrow=2,ncol=3)表示建一个2行3列矩阵 从1到6 先列后行赋值 可用 byrow = T 来更改 可用c给dim赋值行和列数 这样可把一个向量转为一个矩阵 m&lt;-1:6;dim(m)&lt;-c(2,3) 矩阵可以用rbind或cbind生成 t对矩阵转置 因子变量表示分类数据 用标签名区分 用level来命名排序 默认是字母排序 有些函数对顺序敏感可用 levels = c() 来命名 ( 例如低中高的排序 ) 数字表示 drop = T 表示显示截取数据的水平 nlevels给出个数 NaN表未定义或缺失值 NA表示无意义转换或缺失值 NaN可以是NA反之不可以 NA有数据类型 is.NaN与is.NA 可用来检验 数据框 特殊list 每个元素长度相等 每一列类型相同 矩阵所有数据类型相同 特殊属性row.names 转为矩阵data.matrix 变量名自动转化 可以不同 因子变量保持为字符可以用 I data.frame(x,y,I(c)) 数组 表示更高维度的数据 dim() = c(x,y,z) 三维数组表示一组数 dimnames 给数组命名 数组调用如果只有一行 需要drop = F 否则 不会按照数组分类 ts 产生时间序列对象 .Last.value 引用前一个数值 取整数 用round(x,n) n表示保留几位小数 截取整数 trunc 开平方 sqrt 绝对值 abs 指数函数 exp 自然对数函数 log 以 10 为底的对数函数 log10 三角函数 sin cos tan asin acos atan 常用的逻辑运算符有: 大于 &gt; 小于 &lt; 等于 == 小于或等于 &lt;= 大于或等于 &gt;= 与 &amp; 非 ! 或| 判断向量x中是否与y中元素相等 x %in% y 结果返回逻辑值 sum 求和 prod 求连乘 range 给极值范围 duplicated 给出有重复的值 unique 给出无重复的值 向量操作 union 并集 intersect 交集 setdiff 除了交集的部分 rep 用向量循环生成向量 x &lt;- 1:4 # puts c(1,2,3,4) into x i &lt;- rep(2, 4) # puts c(2,2,2,2) into i y &lt;- rep(x, 2) # puts c(1,2,3,4,1,2,3,4) into y z &lt;- rep(x, i) # puts c(1,1,2,2,3,3,4,4) into z w &lt;- rep(x, x) # puts c(1,2,2,3,3,3,4,4,4,4) into w 整型变量后面加上L x&lt;-10L Inf代表1/0 同样1/Inf运算结果为0 3.4 截取数据 []截取数据 可以用[x,y]提取特定数值 [-1,-2]可剔除第一行第二列 [[]]用来从list或者frame里提取元素 类型固定 可提取序列x[[1]][[3]] 可部分匹配 exact=FALSE $用名字提取元素 可部分匹配 提取矩阵时默认只能提取向量 但可以提取1*1矩阵x[1,2,drop=FALSE] 先用is.NA()提取 用!排除 缺失值可用is.element(x,y)来处理很多表示NA值的数字 返回x %in% y的逻辑值 用complete.cases()提取有效数据用[]提取可用数据 head(x,n) n表示从头截取多少行 tail(x,n) n表示从尾截取多少行 subset(x,f) x表示数据 f表示表达式 条件筛选中获得一个变量多个数值的数据使用 [is.element(x,c(' ',' ',' ')),] 或者[x%in%c(' ',' ',' '),] 使用x == c( ' ' , ' ' , ' ' ) 会报错 循环查找三个变量 x!='t' 可能会把空白值输入 应该使用is.element(x,'t') ifelse(con,yes,no) 利用条件筛选 返回yes 或者no 的值 支持正则表达式 3.5 读取数据 read.table read.csv 读取表格 反之write.table readLines 读取文本行 反之writeLines source 读取R代码 反之dump dget 读取多个R代码 反之dput load 读取保存的工作区 反之save unserialize 读取二进制R对象 反之serialize 设置工作目录 getwd() setwd() ?read.table 大数据读取提速 计算内存 comment.char = &quot;&quot; 不扫描注释 设定nrows 设定colClasses initial &lt;- read.table(&quot;datatable.txt&quot;, nrows = 100) classes &lt;- sapply(initial, class) tabAll &lt;- read.table(&quot;datatable.txt&quot;, colClasses = classes) 使用connections与file等保存外部文件指向 3.6 控制结构 if else 条件 if(&lt;condition&gt;) { ## do something } else { ## do something else } if(&lt;condition1&gt;) { ## do something } else if(&lt;condition2&gt;) { ## do something different } else { ## do something different } `for‵ 执行固定次数的循环 嵌套不超过2层 for(i in 1:10) { print(i) } while 条件为真执行循环 条件从左到右执行 count &lt;- 0 while(count &lt; 10) { print(count) count &lt;- count + 1 } repeat 执行无限循环 配合break 中断并跳出循环 next 跳出当前循环继续执行 for(i in 1:100) { if(i &lt;= 20) { ## Skip the first 20 iterations next } ## Do something here } return 退出函数 避免使用无限循环 可用apply替代 3.7 函数 f &lt;- function(&lt;arguments&gt;) { ## Do something interesting } 函数中参数默认值可用formals()显示 参数匹配 先检查命名参数 然后检查部分匹配 最后检查位置匹配 定义函数时可以定义默认值或者设为NULL 懒惰执行：只执行需要执行的语句 ... 向其他函数传参 之后参数不可部分匹配 3.8 编程标准 使用文本文档与文本编辑器 使用缩进 限制代码行宽 80为宜 限制单个函数长度 3.9 范围规则 自由变量采用静态搜索 环境是由数值符号对组成 每个环境都有母环境 函数与环境组成环境闭包 首先从函数环境中寻找变量 之后搜索母环境 最高层为工作区 之后按搜寻列表从扩展包中寻找变量 最后为空环境 之后报错 可以函数内定义函数 S都存在工作区 函数定义一致 R存在内存 可根据需要调用函数环境 3.10 向量化操作 向量操作针对元素 矩阵操作也针对元素 %*% 表示矩阵操作 3.11 日期与时间 日期以data类型存储 时间以POSIXct 或 POSIXlt 类型存储 数字上是从1970-01-01以来的天数或秒数 POSIXct以整数存储时间 POSIXlt以年月日时分秒等信息存储时间 strptime as.Date as.POSIXlt as.POSIXct用来更改字符为时间 3.12 循环 3.12.1 lapply 对列表对象元素应用函数 可配合匿名函数使用 x &lt;- list(a = 1:5, b = rnorm(10)) lapply(x, mean) ## $a ## [1] 3 ## ## $b ## [1] -0.08109674 x &lt;- 1:4 lapply(x, runif, min = 0, max = 10) ## [[1]] ## [1] 5.796912 ## ## [[2]] ## [1] 3.953242 5.186322 ## ## [[3]] ## [1] 7.034730 7.817786 5.435651 ## ## [[4]] ## [1] 1.568902 3.944565 9.858016 2.435848 x &lt;- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2)) lapply(x, function(elt) elt[,1]) ## $a ## [1] 1 2 ## ## $b ## [1] 1 2 3 3.12.2 sapply lapply的精简版 如果结果是单元素列表 转化为向量 如果结果是等长向量 转化为矩阵 否则输出依旧为列表 x &lt;- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5)) sapply(x, mean) ## a b c d ## 2.5000000 0.3577784 0.8948256 5.1196524 3.12.3 vapply 类似lapply可用更复杂函数 返回矩阵 3.12.4 replicate 用于将函数循环使用 如返回随机矩阵 3.12.5 rapply 用how来调整输出方法 如选取某列表中类型数据进行迭代 3.12.6 apply 数组边际函数 常用于矩阵的行列处理 行为1，列为2 可用rowSums rowMeans colSums colMeans 来替代 大数据量更快 x &lt;- matrix(rnorm(50), 10, 5) apply(x, 1, quantile, probs = c(0.25, 0.75)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## 25% -0.3038329 -0.3937917 -0.22144056 -0.65920555 0.3971791 0.1978365 ## 75% 0.5411571 1.1990129 0.02538036 -0.05007987 0.6496955 0.7757481 ## [,7] [,8] [,9] [,10] ## 25% -1.290415 -1.236339 -1.046168 -0.5699054 ## 75% 1.152004 0.790155 0.402795 0.6007617 a &lt;- array(rnorm(2 * 2 * 10), c(2, 2, 10)) apply(a, c(1, 2), mean) ## [,1] [,2] ## [1,] 0.1369942 -0.1137526 ## [2,] -0.1803716 -0.4263440 3.12.7 tapply 对数据子集（因子变量区分）向量应用函数 x &lt;- c(rnorm(10), runif(10), rnorm(10, 1)) f &lt;- gl(3, 10) tapply(x, f, mean) ## 1 2 3 ## 0.8192344 0.4629920 0.9861483 3.12.8 by 对数据按照因子变量应用函数 类似tapply 按照某个分类变量a分类求均值 by(x[,-a],a,mean) 3.12.9 split 将数据按因子分割为列表 常配合lapply使用 类似tapply 可用来生成分组 用drop来删除空分组 x &lt;- c(rnorm(10), runif(10), rnorm(10, 1)) f &lt;- gl(3, 10) lapply(split(x, f), mean) ## $`1` ## [1] -0.1448546 ## ## $`2` ## [1] 0.2630232 ## ## $`3` ## [1] 1.230848 x &lt;- rnorm(10) f1 &lt;- gl(2, 5) f2 &lt;- gl(5, 2) str(split(x, list(f1, f2), drop = TRUE)) ## List of 6 ## $ 1.1: num [1:2] 2.085 -0.352 ## $ 1.2: num [1:2] -0.591 -0.21 ## $ 1.3: num -0.273 ## $ 2.3: num -2.35 ## $ 2.4: num [1:2] 1.66 1.06 ## $ 2.5: num [1:2] 2.163 -0.896 3.12.10 mapply 多变量版apply 从多个参数范围取值 并用函数得到结果 noise &lt;- function(n, mean, sd) { rnorm(n, mean, sd) } mapply(noise, 1:5, 1:5, 2) ## [[1]] ## [1] 1.362567 ## ## [[2]] ## [1] 0.291224 3.559564 ## ## [[3]] ## [1] 3.379793 1.781875 4.377220 ## ## [[4]] ## [1] 1.830211 3.746968 7.535139 5.049241 ## ## [[5]] ## [1] 6.737502 6.274074 3.421209 5.135345 6.517773 #等同于如下循环 #list(noise(1, 1, 2), noise(2, 2, 2), # noise(3, 3, 2), noise(4, 4, 2), # noise(5, 5, 2)) 3.12.11 eapply 对环境变量应用函数 用于包 3.13 模拟 在某分布下产生随机数 d 分布概率密度 r 分布随机数 p 分布累计概率 q 分布分位数 dnorm(x, mean = 0, sd = 1, log = FALSE) pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) rnorm(n, mean = 0, sd = 1) set.seed保证重现性 sample对数据采样 3.14 调试 三种提示 message warning error 只有error致命 关注重现性 调试工具 traceback debug browser trace recover 三思而行 3.15 分析代码 先设计 后优化 system.time 计算代码运行时间 返回对象类型proc_time ‵user time` 执行代码用时 system time CPU时间 elapsed time 实际用时 在多核或并行条件下实际用时可以短于执行代码用时 明确知道耗时较长的函数时使用 Rprof R代码要支持分析函数 summaryRprof可使结果易读 不要与system.time混用 0.02s记录一次执行函数 by.total 记录单个函数用时 by.self 记录函数执行时被调用函数用时 "],
["section-4.html", "Notes 4 数据获取与整理 4.1 概述 4.2 下载 4.3 读取本地文件 4.4 读取excle文件 4.5 读取XML文件 4.6 读取json文件 4.7 读取MySQL数据库 4.8 读取HDF5数据 4.9 读取网页数据 4.10 读取API 4.11 读取其他资源 4.12 数据截取与排序 4.13 数据总结 4.14 数据整理 4.15 数据操作data.table包 4.16 文本处理 4.17 日期处理", " Notes 4 数据获取与整理 4.1 概述 Raw data -&gt; Processing script -&gt; tidy data 前期需求 原始数据 干净数据 code book 详尽的处理步骤记录 原始数据要求 未经处理 未经修改 未经去除异常值 未经总结 干净数据 每个变量一列 同一变量不同样本不在一行 一种变量一个表 多张表要有一列可以相互链接 有表头 变量名要有意义 一个文件一张表 code book 变量信息 总结方式 实验设计 文本文件 包含研究设计与变量信息的章节 处理步骤记录 脚本文件 输入为原始数据 输出为处理过数据 脚本中无特定参数 4.2 下载 设定工作目录与数据存储目录 if (!file.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } url下载与时间记录 fileUrl &lt;- &quot;yoururl&quot; download.file(fileUrl, destfile = &quot;./data/XXX.csv&quot;, method = &quot;curl&quot;) list.files(&quot;./data&quot;) dateDownloaded &lt;- date() 4.3 读取本地文件 read.table read.csv 默认sep=&quot;,&quot;, header=TRUE quote 设定引用 na.strings 设定缺失值字符 nrows 设定读取字段 skip 跳过开始行数 4.4 读取excle文件 xlsx包 library(xlsx) cameraData &lt;- read.xlsx(&quot;./data/cameras.xlsx&quot;,sheetIndex=1,header=TRUE) head(cameraData) # read.xlsx2更快不过选行读取时会不稳定 # 支持底层读取 如字体等 XLConnect包 library(XLConnect) wb &lt;- loadWorkbook(&quot;XLConnectExample1.xlsx&quot;, create = TRUE) createSheet(wb, name = &quot;chickSheet&quot;) writeWorksheet(wb, ChickWeight, sheet = &quot;chickSheet&quot;, startRow = 3, startCol = 4) saveWorkbook(wb) # 支持区域操作 生成报告 图片等 4.5 读取XML文件 网页常用格式 形式与内容分开 形式包括标签 元素 属性等 XML包 library(XML) fileUrl &lt;- &quot;http://www.w3schools.com/xml/simple.xml&quot; # 读取xml结构 doc &lt;- xmlTreeParse(fileUrl,useInternal=TRUE) # 提取节点 rootNode &lt;- xmlRoot(doc) # 提取根节点名 xmlName(rootNode) # 提取子节点名 names(rootNode) # 提取节点数值 xmlSApply(rootNode,xmlValue) XPath XML的一种查询语法 /node 顶级节点 //node 所有子节点 node(???) 带属性名的节点 node(??? =“bob”) 属性名为bob的节点 # 提取节点下属性名为name的数值 xpathSApply(rootNode,&quot;//name&quot;,xmlValue) 4.6 读取json文件 js对象符号 结构化 常作为API输出格式 jsonlite包 library(jsonlite) # 读取json文件 jsonData &lt;- fromJSON(&quot;https://api.github.com/users/jtleek/repos&quot;) # 列出文件名 names(jsonData) # 可嵌套截取 jsonData$owner$login # 可将R对象写成json文件 myjson &lt;- toJSON(iris, pretty=TRUE) 4.7 读取MySQL数据库 网络应用常见数据库软件 一行一记录 数据库表间有index向量 常见命令 指南 RMySQL包 library(RMySQL) # 读取数据库 ucscDb &lt;- dbConnect(MySQL(),user=&quot;genome&quot;, host=&quot;genome-mysql.cse.ucsc.edu&quot;) result &lt;- dbGetQuery(ucscDb,&quot;show databases;&quot;); # 断开链接 dbDisconnect(ucscDb); # 读取指定数据库 hg19 &lt;- dbConnect(MySQL(),user=&quot;genome&quot;, db=&quot;hg19&quot;, host=&quot;genome-mysql.cse.ucsc.edu&quot;) allTables &lt;- dbListTables(hg19) length(allTables) # mysql语句查询 dbGetQuery(hg19, &quot;select count(*) from affyU133Plus2&quot;) # 选择子集 query &lt;- dbSendQuery(hg19, &quot;select * from affyU133Plus2 where misMatches between 1 and 3&quot;) affyMis &lt;- fetch(query); quantile(affyMis$misMatches) 4.8 读取HDF5数据 分层分组读取大量数据的格式 rhdf5包 library(rhdf5) created = h5createFile(&quot;example.h5&quot;) created = h5createGroup(&quot;example.h5&quot;,&quot;foo&quot;) created = h5createGroup(&quot;example.h5&quot;,&quot;baa&quot;) created = h5createGroup(&quot;example.h5&quot;,&quot;foo/foobaa&quot;) h5ls(&quot;example.h5&quot;) A = matrix(1:10,nr=5,nc=2) h5write(A, &quot;example.h5&quot;,&quot;foo/A&quot;) B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2)) attr(B, &quot;scale&quot;) &lt;- &quot;liter&quot; h5write(B, &quot;example.h5&quot;,&quot;foo/foobaa/B&quot;) h5ls(&quot;example.h5&quot;) df = data.frame(1L:5L,seq(0,1,length.out=5), c(&quot;ab&quot;,&quot;cde&quot;,&quot;fghi&quot;,&quot;a&quot;,&quot;s&quot;), stringsAsFactors=FALSE) h5write(df, &quot;example.h5&quot;,&quot;df&quot;) h5ls(&quot;example.h5&quot;) readA = h5read(&quot;example.h5&quot;,&quot;foo/A&quot;) readB = h5read(&quot;example.h5&quot;,&quot;foo/foobaa/B&quot;) readdf= h5read(&quot;example.h5&quot;,&quot;df&quot;) 4.9 读取网页数据 网页抓取HTML数据 读完了一定关链接 httr包 con = url(&quot;http://scholar.google.com/citations?user=HI-I6C0AAAAJ&amp;hl=en&quot;) htmlCode = readLines(con) close(con) htmlCode library(XML) url &lt;- &quot;http://scholar.google.com/citations?user=HI-I6C0AAAAJ&amp;hl=en&quot; html &lt;- htmlTreeParse(url, useInternalNodes=T) xpathSApply(html, &quot;//title&quot;, xmlValue) library(httr) html2 = GET(url) content2 = content(html2,as=&quot;text&quot;) parsedHtml = htmlParse(content2,asText=TRUE) xpathSApply(parsedHtml, &quot;//title&quot;, xmlValue) GET(&quot;http://httpbin.org/basic-auth/user/passwd&quot;) GET(&quot;http://httpbin.org/basic-auth/user/passwd&quot;, authenticate(&quot;user&quot;,&quot;passwd&quot;)) google = handle(&quot;http://google.com&quot;) pg1 = GET(handle=google,path=&quot;/&quot;) pg2 = GET(handle=google,path=&quot;search&quot;) 4.10 读取API 通过接口授权后调用数据 httr包 myapp = oauth_app(&quot;twitter&quot;, key=&quot;yourConsumerKeyHere&quot;,secret=&quot;yourConsumerSecretHere&quot;) sig = sign_oauth1.0(myapp, token = &quot;yourTokenHere&quot;, token_secret = &quot;yourTokenSecretHere&quot;) homeTL = GET(&quot;https://api.twitter.com/1.1/statuses/home_timeline.json&quot;, sig) json1 = content(homeTL) json2 = jsonlite::fromJSON(toJSON(json1)) 4.11 读取其他资源 图片 jpeg readbitmap png EBImage (Bioconductor) GIS rdgal rgeos raster 声音 tuneR seewave 4.12 数据截取与排序 增加行直接$ seq产生序列 通过[按行 列或条件截取 which返回行号 排序向量用sort 排序数据框(多向量)用order plyl包排序 library(plyr) arrange(X,var1) arrange(X,desc(var1)) 4.13 数据总结 head tail查看数据 summary str总结数据 quantile 按分位数总结向量 table 按向量元素频数总结 sum(is.na(data)) any(is.na(data)) all(data$x &gt; 0) 异常值总结 colSums(is.na(data)) 行列求和 table(data$x %in% c(&quot;21212&quot;))特定数值计数总结 xtabs ftable 创建列联表 print(object.size(fakeData),units=&quot;Mb&quot;) 现实数据大小 cut 通过设置breaks产生分类变量 Hmisc包 library(Hmisc) data$zipGroups = cut2(data$zipCode,g=4) table(data$zipGroups) library(plyr) # mutate进行数据替换或生成 data2 = mutate(data,zipGroups=cut2(zipCode,g=4)) table(data2$zipGroups) 4.14 数据整理 每一列一个变量 每一行一个样本 每个文件存储一类样本 melt进行数据融合 reshape2包 dcast分组汇总数据框 acast分组汇总向量数组 arrange指定变量名排序 merge按照指定向量合并数据 plyr包的join函数也可实现合并 4.15 数据操作data.table包 基本兼容data.frame 速度更快 通过key可指定因子变量并快速提取分组的行 可在第二个参数是R表达式 DT[,list(mean(x),sum(z))] DT[,table(y)] 可用:生成新变量 进行简单计算 DT[,w:=z^2] DT[,m:= {tmp &lt;- (x+z); log2(tmp+5)}] 进行数据条件截取 DT[,a:=x&gt;0] DT[,b:= mean(x+w),by=a] 进行计数 DT &lt;- data.table(x=sample(letters[1:3], 1E5, TRUE)) DT[, .N, by=x] 4.16 文本处理 处理大小写tolower toupper 处理变量名strsplit firstElement &lt;- function(x){x[1]} sapply(splitNames,firstElement) 字符替换sub gsub 寻找变量grep(返回行号) grepl(返回逻辑值) stringr包 stringr paste0 不带空格 str_trim 去除空格 命名原则 变量名小写 描述性 无重复 变量名不要符号分割 Names of variables should be 正则表达式 文字处理格式 ^ 匹配开头 $ 匹配结尾 [] 匹配大小写 ^在开头表示非 . 匹配任意字符 | 匹配或 () 匹配与 ? 匹配可选择 * 匹配任意 + 匹配至少一个 {} 匹配其中最小最大 一个值表示精确匹配 m,表示至少m次匹配 \\1 匹配前面指代 4.17 日期处理 formate处理日期格式 %d 日 %a 周缩写 %A 周 %m 月 %b 月缩写 %B 月全名 %y 2位年 %Y 4位年 weekdays 显示星期 months 显示月份 julian 显示70年以来的日期 lubridate包 ymd mdy dmy ymd_hms Sys.timezone "],
["section-5.html", "Notes 5 探索性数据分析 5.1 探索绘图原则 5.2 探索性绘图 5.3 R绘图系统 5.4 分层聚类 5.5 k-means聚类 5.6 维度还原", " Notes 5 探索性数据分析 5.1 探索绘图原则 表示可比的对比 表示因果 解释 机制 系统结构 表示多元变量（超过2） 证据整合 目的驱动非工具驱动 证据描述要标注限定恰当 内容为王 5.2 探索性绘图 个人理解用 不用过分关注细节 基于问题或假设出发 5.3 R绘图系统 5.3.1 基础包 艺术家绘画模式 graphics 包括基础包的绘图函数如plot, hist, boxplot grDevices 包括执行调用绘图设备函数如X11, PDF, PostScript, PNG 叠加函数 高度自由度 初始化新图 然后标注 以下命令熟记 pch: the plotting symbol (default is open circle) lty: the line type (default is solid line), can be dashed, dotted, etc. lwd: the line width, specified as an integer multiple col: the plotting color, specified as a number, string, or hex code; the colors() function gives you a vector of colors by name xlab: character string for the x-axis label ylab: character string for the y-axis label par():查找做图的画布参数 具体如下 las: the orientation of the axis labels on the plot bg: the background color mar: the margin size oma: the outer margin size (default is 0 for all sides) mfrow: number of plots per row, column (plots are filled row-wise) mfcol: number of plots per row, column (plots are filled column-wise) plot: make a scatterplot, or other type of plot depending on the class of the object being plotted lines: add lines to a plot, given a vector x values and a corresponding vector of y values (or a 2-column matrix); this function just connects the dots points: add points to a plot text: add text labels to a plot using specified x, y coordinates title: add annotations to x, y axis labels, title, subtitle, outer margin mtext: add arbitrary text to the margins (inner or outer) of the plot axis: adding axis ticks/labels 图形设备 图像一定要有设备 屏幕设备 Mac quartz() windows windows() Unix/linux x11() 先调用后用dev.off()关闭设备 矢量图设备 保真放大 元素过多体积庞大 pdf() svg() winmetafile() postscript() 位图设备 放大失真 基于像素 png() jpeg() tiff() bmp() 当前设备dev.cur() 设置设备dev.set(&lt;integer&gt;) 设备转移dev.copy dev.copy2pdf 5.3.2 lattice 一站式解决 lattice 包括框架图函数如xyplot, bwplot, levelplot grid 包括独立于基础绘图系统的网格绘图系统 一个函数解决问题 默认自定义空间少 返回trellis类型对象 可单独存储 界面调整使用panel选项 以下为常见函数 xyplot: this is the main function for creating scatterplots bwplot: box-and-whiskers plots (“boxplots”) histogram: histograms stripplot: like a boxplot but with actual points dotplot: plot dots on “violin strings” splom: scatterplot matrix; like pairs in base plotting system levelplot, contourplot: for plotting “image” data 基本格式 xyplot(y ~ x | f * g, data) 可同时展示分组信息及交互作用 5.3.3 ggplot2 基于图形语法理念 图形属性映射数据问题 自动处理界面 允许后期添加 结合base与lattice 默认友好 基础绘图qplot() ggplot() 通过叠加元素出图 细节调整xlab(), ylab(), labs(), ggtitle() 主题调整theme() 做图需求 数据框 data.frame 属性映射 asethetic mappling 几何对象 geoms 条件 facets 统计转换 stats 范围量表 scales 坐标轴系统 coordinate system 5.3.4 数学绘图 Tex语法 使用expression() ?plotmath 5.3.5 色彩管理 colorRamp 返回01间数值 表示颜色过度 colorRampPalette 返回8位颜色代码调色盘 colors 返回可用颜色 RColorBrewer包 含有预先配色信息 序列 无序 两级 rgb产生三原色颜色 alpha 控制透明度 绘图时用col调用调色盘颜色 pal &lt;- colorRamp(c(&quot;red&quot;, &quot;blue&quot;)) pal(0) ## [,1] [,2] [,3] ## [1,] 255 0 0 pal(1) ## [,1] [,2] [,3] ## [1,] 0 0 255 pal(0.5) ## [,1] [,2] [,3] ## [1,] 127.5 0 127.5 ##### pal &lt;- colorRampPalette(c(&quot;red&quot;, &quot;yellow&quot;)) pal(2) ## [1] &quot;#FF0000&quot; &quot;#FFFF00&quot; pal(10) ## [1] &quot;#FF0000&quot; &quot;#FF1C00&quot; &quot;#FF3800&quot; &quot;#FF5500&quot; &quot;#FF7100&quot; &quot;#FF8D00&quot; &quot;#FFAA00&quot; ## [8] &quot;#FFC600&quot; &quot;#FFE200&quot; &quot;#FFFF00&quot; ##### library(RColorBrewer) cols &lt;- brewer.pal(3, &quot;BuGn&quot;) 5.4 分层聚类 找到最近的 聚到一起 找下个最近的 给出距离范围与距离计算方法 欧氏距离 多维空间点距 开平方 manhattan距离 出租车距离 绝对值 给出变量间或样本间的关系 图形可能不稳定 多少样本多少类 结果是确定的 选定cut点并不明显 应该首先用来探索 5.5 k-means聚类 固定聚类数 给出聚类中心 寻找最近的点 循环 需要聚类数与聚类距离范围 需要大量聚类 通过眼睛 交叉检验 k的经验数值\\(\\sqrt{n/2}\\) 或者根据解释的变量变化多少来选取 结果不确定 根据聚类数与迭代次数而变化 5.6 维度还原 找到最不相关的数来解释整体方差（统计）在这些数中选取个数最少的来解释原始数据（压缩） 不一定是真实向量的叠加 SVD是PCA的一种解法 UDV三个向量 其中U表示行变化模式 D表示方差 V表示列变换模式 这样有助于解释主成分变化 标准化与否影响结果 计算量大 类似探索分析还有因子分析 独立成分分析 潜在语义分析 impute包可补充缺失值 "],
["section-6.html", "Notes 6 可复算性研究 6.1 Replication 6.2 Reproducible 6.3 研究流程 6.4 数据分析步骤 6.5 数据分析文件结构 6.6 文本化统计编程-Knitr 6.7 结果通讯 6.8 检查列表 6.9 基于证据的数据分析", " Notes 6 可复算性研究 6.1 Replication 科学研究的的终极标准是研究证据可独立发现与验证 并非所有结果都可以重复 6.2 Reproducible 可重复的数据分析过程与代码 数据维度增高 现有数据可被整合入更大的数据集 计算机条件允许 6.3 研究流程 流程图 6.4 数据分析步骤 定义问题 背后要有科学假设或问题 从大到小 具体定义 定义理想数据 描述性的 &lt;- 总体数据 探索性的 &lt;- 有属性测量的样本数据 推断性的 &lt;- 合适的总体 随机采样 预测性的 &lt;- 来自同一总体 有训练集与测试集的样本 因果性的 &lt;- 随机性研究 机械性的 &lt;- 系统中所有组成部分的数据 决定可获取数据 网络免费数据 购买数据 注意使用条款 数据不存在 自己创造 &lt;- 实验 获取数据 原始数据 引用来源 网络数据注明数据来源URL与获取时间 整理数据 原始数据需要整理 如果事先处理过要搞清楚如何处理的 了解数据来源 需要重新格式化 采样 &lt;- 记录步骤 判断数据是否合适 不合适重新获取 探索性数据分析 描述性总结数据 检查缺失值 绘制探索性图 尝试探索性分析 例如聚类 统计预测/建模 基于探索性分析 根据问题确定方法 数据转换要解释 测定的不确定性要考虑 解释结果 描述 相关 推断 预测 质疑结果 问题 数据源 处理过程 分析 结论 整合写出结果 从问题角度出发 形成一个故事 不要包含分析过程除非用来说明问题 消除质疑 以故事而不是时间顺序描述 图片要漂亮 写出可重复的R代码 Rmarkdown文件 6.5 数据分析文件结构 Data Raw data 来自网络在Readme里注明url 描述 日期 Processed data 命名体现处理过程 Readme里注明处理过程 Figures Exploratory figures 不必考虑装饰 Final figures 只考虑装饰 R code Raw scripts 不必过分注释 版本控制 不一定用得上 Final scripts 注释清晰 包括处理细节 只包括文章需要费分析 R Markdown files (optional) Text Readme files 按步骤记录清晰 Text of analysis 包括前言 方法 结果 结论 讲故事 有引用 6.6 文本化统计编程-Knitr markdown是轻量化结构语言 R markdown 是轻量化统计结构语言 文本+代码块 逻辑清晰 文本语言可用latex markdown 代码块可用R 不用保存输出 可缓存结果 cacher包 6.7 结果通讯 研究论文的信息层级 题目/作者名单 摘要 主体/结果 支持材料/细节 代码/数据 邮件汇报的信息层级 题目最好一行一句 描述问题 如何实验 总结发现 简明扼要 如果有问题 写成yes/no形式 附件齐全严谨 6.8 检查列表 数据选取得当 问题简单专一 队友靠谱 兴趣驱动 不要手动处理数据 全部交给计算机 少用交互界面 用命令行界面并记录历史 使用版本控制 处理降速而冷静 记录软件操作环境 sessionInfo() 不保存结果保证数据可重复 使用随机数要说明种子 原始数据-处理数据-分析-报告 考虑从哪一步开始数据重复性变差 6.9 基于证据的数据分析 可重复性研究不保证结果是对的 发表后研究存在动因 应关注数据生成前的过程 设定基于证据研究的路线图 减少研究人员的自由度 提出区域研究范式 "],
["-1.html", "Notes 7 统计推断 7.1 导论 7.2 概率 7.3 期望 7.4 方差 7.5 独立性 7.6 条件概率 7.7 贝叶斯定理 7.8 常见分布 7.9 渐进 7.10 T 置信区间 7.11 似然函数 7.12 贝叶斯推断 7.13 两独立样本t检验 7.14 假设检验 7.15 P 值 7.16 功效 7.17 多重比较 7.18 重采样推断", " Notes 7 统计推断 7.1 导论 定义 用需要考虑不确定度的含噪音的统计学数据推断事实 工具 随机化 随机采样 采样模型 假设检验 置信区间 概率模型 实验设计 bootstraping 排列交换随机 类型 频率派 使用概率的频率解释来控制错误率 贝叶斯派 给定概率与数据概率哪个靠谱 7.2 概率 术语 样本空间 Ω 事件 样本空间子集 E 单独事件 ω 空事件 ∅ \\(ω∈E\\) ω发生E发生 \\(ω∉E\\) ω发生E不发生 \\(E⊂F\\) E发生则F发生 \\(E∩F\\) EF一起发生 \\(E∪F\\) EF中至少一个发生 \\(E∩F=∅\\) EF互斥 \\(E^c\\) 或 \\(\\bar E\\) E不发生 概率 对事件 \\(E\\subset \\Omega\\), \\(0 \\leq P(E) \\leq 1\\) \\(P(\\Omega) = 1\\) 如果 \\(E_1\\) 与 \\(E_2\\) 互斥 有\\(P(E_1 \\cup E_2) = P(E_1) + P(E_2)\\). 概率无限可加性 \\(P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n P(A_i)\\) \\(P(\\emptyset) = 0\\) \\(P(E) = 1 - P(E^c)\\) \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) 如果 \\(A \\subset B\\) 则 \\(P(A) \\leq P(B)\\) \\(P\\left(A \\cup B\\right) = 1 - P(A^c \\cap B^c)\\) \\(P(A \\cap B^c) = P(A) - P(A \\cap B)\\) \\(P(\\cup_{i=1}^n E_i) \\leq \\sum_{i=1}^n P(E_i)\\) \\(P(\\cup_{i=1}^n E_i) \\geq \\max_i P(E_i)\\) 随机变量 实验的数值输出 离散随机变量取可数的概率 \\(P(X = k)\\) 连续随机变量取连续区间子集概率 \\(P(X \\in A)\\) 概率质量函数（PMF）&lt;- 离散随机变量 对于所有 \\(x\\) \\(p(x) \\geq 0\\) \\(\\sum_{x} p(x) = 1\\) 概率密度函数（PDF）&lt;- 连续随机变量 对于所有 \\(x\\) \\(f(x) \\geq 0\\) \\(f(x)\\) 下面积为1 累计概率函数（CDF） 定义 \\(F(x) = P(X \\leq x)\\) 生存函数 \\(S(x) = P(X &gt; x)\\) \\(S(x) = 1 - F(x)\\) 对于连续函数 CDF是PDF的积分 分位数 \\(\\alpha^{th}\\) \\(F(x_\\alpha) = \\alpha\\) \\(50^{th}\\) 分位数是中位数 7.3 期望 离散随机变量均值 \\(E[X] = \\sum_x xp(x)\\) \\(E[X]\\) 代表质量与位置的中心 \\(\\{x, p(x)\\}\\) 连续随机变量均值 \\(E[X] = \\mbox{the area under the function}~~~ t f(t)\\) 期望值是线性可加的 如果 \\(a\\) 与 \\(b\\) 不随机 \\(X\\) 与 \\(Y\\) 是随机变量 \\(E[aX + b] = a E[X] + b\\) \\(E[X + Y] = E[X] + E[Y]\\) 样本均值是总体均值\\(\\mu\\)的无偏估计的证明 \\[ \\begin{eqnarray*} E\\left[ \\frac{1}{n}\\sum_{i=1}^n X_i\\right] &amp; = &amp; \\frac{1}{n} E\\left[\\sum_{i=1}^n X_i\\right] \\\\ &amp; = &amp; \\frac{1}{n} \\sum_{i=1}^n E\\left[X_i\\right] \\\\ &amp; = &amp; \\frac{1}{n} \\sum_{i=1}^n \\mu = \\mu. \\end{eqnarray*} \\] 7.4 方差 描述随机变量的离散情况 如果 \\(X\\) 是均值 \\(\\mu\\) 的随机变量 其方差为\\(Var(X) = E[(X - \\mu)^2]\\) 离开均值距离期望的平方 计算公式 \\(Var(X) = E[X^2] - E[X]^2\\) 如果 \\(a\\) 是常数有 \\(Var(aX) = a^2 Var(X)\\) 方差的开方是标准差 单位与 \\(X\\) 一致 车比雪夫不等式（Chebyshev’s inequality）边界极为保守 \\[ P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} \\] 7.5 独立性 独立事件 两事件 \\(A\\) 与 \\(B\\) 在 \\(P(A \\cap B) = P(A)P(B)\\) 下独立 在 \\(P([X \\in A] \\cap [Y \\in B]) = P(X\\in A)P(Y\\in B)\\) 下两随机变量 \\(X\\) 与 \\(Y\\) 独立 对于一组随机独立变量\\(X_1, X_2, \\ldots, X_n\\)有 \\(f(x_1,\\ldots, x_n) = \\prod_{i=1}^n f_i(x_i)\\) iid随机变量（independent and identically distributed） 来自同一分布相互独立的随机变量 协方差（covariance） \\(Cov(X, Y) = E[(X - \\mu_x)(Y - \\mu_y)] = E[X Y] - E[X]E[Y]\\) \\(Cov(X, Y) = Cov(Y, X)\\) \\(Cov(X, Y)\\) 可以有正负 \\(|Cov(X, Y)| \\leq \\sqrt{Var(X) Var(y)}\\) 相关性（correlation） \\(X\\) 与 \\(Y\\) 的相关性 \\(Cor(X, Y) = Cov(X, Y) / \\sqrt{Var(X) Var(y)}\\) \\(-1 \\leq Cor(X, Y) \\leq 1\\) 只有对常数 \\(a\\) 与 \\(b\\)满足 \\(X = a + bY\\) 时\\(Cor(X, Y) = \\pm 1\\) \\(Cor(X, Y)\\) 无单位 \\(Cor(X, Y) = 0\\) 时 \\(X\\) 与 \\(Y\\) 不相关 \\(Cor(X,Y)\\) 越接近1 \\(X\\) 与 \\(Y\\) 越正相关 反之接近-1 负相关 \\(\\{X_i\\}_{i=1}^n\\) 是一组随机变量 当 \\(\\{X_i\\}\\) 不相关时 \\(Var\\left(\\sum_{i=1}^n a_i X_i + b\\right) = \\sum_{i=1}^n a_i^2 Var(X_i)\\) 如果一组随机变量\\(\\{X_i\\}\\)不相关 方差的和等于和的方差 非标准差 样本均值方差的推导 \\[ \\begin{eqnarray*} Var(\\bar X) &amp; = &amp; Var \\left( \\frac{1}{n}\\sum_{i=1}^n X_i \\right)\\\\ \\\\ &amp; = &amp; \\frac{1}{n^2} Var\\left(\\sum_{i=1}^n X_i \\right)\\\\ \\\\ &amp; = &amp; \\frac{1}{n^2} \\sum_{i=1}^n Var(X_i) \\\\ \\\\ &amp; = &amp; \\frac{1}{n^2} \\times n\\sigma^2 \\\\ \\\\ &amp; = &amp; \\frac{\\sigma^2}{n} \\end{eqnarray*} \\] 当 \\(X_i\\) 独立且方差为 \\(Var(\\bar X) = \\frac{\\sigma^2}{n}\\) \\(\\sigma/\\sqrt{n}\\) 为样本均值的标准误 样本均值的标准误就是样本均值分布的标准差 \\(\\sigma\\) 是一次观察分布的标准差 样本均值要比一次观察变化小 因此除以\\(\\sqrt{n}\\) 样本方差 \\(S^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar X)^2}{n-1}\\) 总体方差 \\(\\sigma^2\\)的估计 计算 \\(\\sum_{i=1}^n (X_i - \\bar X)^2 = \\sum_{i=1}^n X_i^2 - n \\bar X^2\\) 均值偏差平方的均值 样本方差是总体方差的无偏估计 \\[ \\begin{eqnarray*} E\\left[\\sum_{i=1}^n (X_i - \\bar X)^2\\right] &amp; = &amp; \\sum_{i=1}^n E\\left[X_i^2\\right] - n E\\left[\\bar X^2\\right] \\\\ \\\\ &amp; = &amp; \\sum_{i=1}^n \\left\\{Var(X_i) + \\mu^2\\right\\} - n \\left\\{Var(\\bar X) + \\mu^2\\right\\} \\\\ \\\\ &amp; = &amp; \\sum_{i=1}^n \\left\\{\\sigma^2 + \\mu^2\\right\\} - n \\left\\{\\sigma^2 / n + \\mu^2\\right\\} \\\\ \\\\ &amp; = &amp; n \\sigma^2 + n \\mu ^ 2 - \\sigma^2 - n \\mu^2 \\\\ \\\\ &amp; = &amp; (n - 1) \\sigma^2 \\end{eqnarray*} \\] 澄清 假定 \\(X_i\\) 是 iid 均值 \\(\\mu\\) 方差 \\(\\sigma^2\\) \\(S^2\\) 估计 \\(\\sigma^2\\) \\(S^2\\) 的计算涉及除 \\(n-1\\) \\(S / \\sqrt{n}\\) 估计 \\(\\sigma / \\sqrt{n}\\) 是均值的标准误 7.6 条件概率 \\(B\\) 为一个事件 有 \\(P(B) &gt; 0\\) \\(B\\) 出现条件下 \\(A\\) 的条件概率为 \\(P(A ~|~ B) = \\frac{P(A \\cap B)}{P(B)}\\) 如果 \\(A\\) 与 \\(B\\) 独立 有 \\(P(A ~|~ B) = \\frac{P(A) P(B)}{P(B)} = P(A)\\) 7.7 贝叶斯定理 \\[ P(B ~|~ A) = \\frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}. \\] 2*2 列联表 - 诊断测试 示意图 7.8 常见分布 贝努力分布 二元输出变量 数值为0或1 概率\\(p\\) 与 \\(1-p\\) \\(X\\)的PMF是\\(P(X = x) = p^x (1 - p)^{1 - x}\\) 均值 \\(p\\) 方差 \\(p(1 - p)\\) 如果有iid的贝努力观察\\(x_1,\\ldots, x_n\\) 似然函数 \\(\\prod_{i=1}^n p^{x_i} (1 - p)^{1 - x_i} = p^{\\sum x_i} (1 - p)^{n - \\sum x_i}\\) 似然函数依赖\\(x_i\\)的和 \\(\\sum_i x_i / n\\) 包含了所有 \\(p\\) 的可能性 最大化似然函数可以得到 \\(p\\) 的估计 二项分布 PMF \\[ P(X = x) = \\left( \\begin{array}{c} n \\\\ x \\end{array} \\right) p^x(1 - p)^{n-x} \\] 对于 \\(x=0,\\ldots,n\\) 正态分布 PDF \\((2\\pi \\sigma^2)^{-1/2}e^{-(x - \\mu)^2/2\\sigma^2}\\) \\(X\\) 为均值 \\(E[X] = \\mu\\) 方差 \\(Var(X) = \\sigma^2\\) 的iid随机变量 写作\\(X\\sim \\mbox{N}(\\mu, \\sigma^2)\\) 均值 \\(\\mu = 0\\) 方差 \\(\\sigma = 1\\) 是标准正态分布 标准正态函数写作 \\(\\phi\\) 标准正态随机变量用 \\(Z\\) 表示 如果 \\(X \\sim \\mbox{N}(\\mu,\\sigma^2)\\) 并且 \\(Z = \\frac{X -\\mu}{\\sigma}\\) 是标准正态函数 如果 \\(Z\\) 是标准正态函数 \\(X = \\mu + \\sigma Z \\sim \\mbox{N}(\\mu, \\sigma^2)\\) 非标准正态密度函数 \\(\\phi\\{(x - \\mu) / \\sigma\\}/\\sigma\\) 正态似然函数对方差的估计是有偏的 正态的和是正态 样本均值正态 正态的平方是卡方 泊松分布 PMF \\(P(X = x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\) 均值方差均为 \\(\\lambda\\) 可看做很短时间间隔中发生事件的概率 模拟速率 其中\\(\\lambda * h\\)小于1 则各时间段独立 \\(X \\sim Poisson(\\lambda t)\\) \\(\\lambda = E[X / t]\\)是速率 \\(t\\) 是总时间 \\(n\\) 大 \\(p\\) 小是对二项分布的模拟 \\(X \\sim \\mbox{Binomial}(n, p)\\), \\(\\lambda = n p\\) 7.9 渐进 样本接近无穷大时统计量的行为 频率派的基石 大数理论（LLN） 样本数量越多 均值接近期望 中心极限理论 (CLT) iid 变量均值的分布标准化后随样本数增加接近标准正态分布 \\[ \\frac{\\bar X_n - \\mu}{\\sigma / \\sqrt{n}} = \\frac{\\mbox{Estimate} - \\mbox{Mean of estimate}}{\\mbox{Std. Err. of estimate}} \\] 可根据变量分布来 知道均值 方差 计算出样本均值标准误 就可以根据CLT计算逼近的统计量 置信区间 根据CLT随机区间\\(\\bar X_n \\pm z_{1-\\alpha/2}\\sigma / \\sqrt{n}\\) 包括 \\(\\mu\\) 的概率逼近于 100\\((1-\\alpha)\\)% \\(z_{1-\\alpha/2}\\)为标准正态分布\\(1-\\alpha/2\\)的分位数 \\(100(1 - \\alpha)\\)% 为置信区间 \\(\\sigma\\) 可用样本估计 \\(s\\) 来近似 估计是基于分布假设的 如果分布有解析解 则置信区间可以更准确的得到估计 先生成不依赖参数的统计量 根据统计量的概率分布计算参数的边界 7.10 T 置信区间 卡方分布 假定 \\(S^2\\) 是来自\\(n\\)个 iid \\(N(\\mu,\\sigma^2)\\) 数据样本的方差 有\\(\\frac{(n - 1) S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) 符合自由度\\(n-1\\)的卡方分布 不对称分布 均值是自由度 方差是两倍的自由度 方差的置信区间 \\[ \\begin{eqnarray*} 1 - \\alpha &amp; = &amp; P \\left( \\chi^2_{n-1, \\alpha/2} \\leq \\frac{(n - 1) S^2}{\\sigma^2} \\leq \\chi^2_{n-1,1 - \\alpha/2} \\right) \\\\ \\\\ &amp; = &amp; P\\left(\\frac{(n-1)S^2}{\\chi^2_{n-1,1-\\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{n-1,\\alpha/2}} \\right) \\\\ \\end{eqnarray*} \\] \\(\\left[\\frac{(n-1)S^2}{\\chi^2_{n-1,1-\\alpha/2}}, \\frac{(n-1)S^2}{\\chi^2_{n-1,\\alpha/2}}\\right]\\) 是 \\(\\sigma^2\\) 的 \\(100(1-\\alpha)\\%\\) 置信区间 依赖正态性假设 开方后得到 \\(\\sigma\\) 的置信区间 Gosset的 t 分布 比正态分布尾厚 考虑自由度 自由度大时接近正态分布 \\(\\frac{Z}{\\sqrt{\\frac{\\chi^2}{df}}}\\) 假定 \\((X_1,\\ldots,X_n)\\) 是 iid \\(N(\\mu,\\sigma^2)\\) 有 \\(\\frac{\\bar X - \\mu}{\\sigma / \\sqrt{n}}\\) 是标准正态分布 \\(\\sqrt{\\frac{(n - 1) S^2}{\\sigma^2 (n - 1)}} = S / \\sigma\\) 是卡方除以自由度的开方 有 \\[ \\frac{\\frac{\\bar X - \\mu}{\\sigma /\\sqrt{n}}}{S/\\sigma} = \\frac{\\bar X - \\mu}{S/\\sqrt{n}} \\] 服从自由度\\(n-1\\)的\\(t\\)分布 均值的置信区间 \\[ \\begin{eqnarray*} &amp; &amp; 1 - \\alpha \\\\ &amp; = &amp; P\\left(-t_{n-1,1-\\alpha/2} \\leq \\frac{\\bar X - \\mu}{S/\\sqrt{n}} \\leq t_{n-1,1-\\alpha/2}\\right) \\\\ \\\\ &amp; = &amp; P\\left(\\bar X - t_{n-1,1-\\alpha/2} S / \\sqrt{n} \\leq \\mu \\leq \\bar X + t_{n-1,1-\\alpha/2}S /\\sqrt{n}\\right) \\end{eqnarray*} \\] \\(t_{df,\\alpha}\\) 是t分布的 \\(\\alpha^{th}\\) 分位数 自由度 \\(df\\) t检验不适合有偏分布 置信区间中心也不在均值上 7.11 似然函数 一组数据的似然函数是数据固定下参数的联合概率密度函数 似然函数可用来估计参数 是参数的函数 似然函数比估计两个可能参数值的可能性 给定模型与数据 似然函数包含所有参数可能性 样本独立时 参数的似然函数是各独立样本似然函数的乘积 参数使似然函数概率取最大值时真实的可能性更大 更支持这组数据 这个估计是最大似然估计（MLE） 7.12 贝叶斯推断 \\(\\mbox{Posterior} \\propto \\mbox{Likelihood} \\times \\mbox{Prior}\\) 先验beta分布 01之间 依赖 \\(\\alpha\\) \\(\\beta\\) 的概率密度函数 \\[ \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p ^ {\\alpha - 1} (1 - p) ^ {\\beta - 1} ~~~~\\mbox{for} ~~ 0 \\leq p \\leq 1 \\] 均值 \\(\\alpha / (\\alpha + \\beta)\\) 方差 \\(\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\) \\(\\alpha = \\beta = 1\\) 为均匀分布 后验beta分布 参数\\(\\tilde \\alpha = x + \\alpha\\) \\(\\tilde \\beta = n - x + \\beta\\) 的beta分布 \\[ \\begin{align} \\mbox{Posterior} &amp;\\propto p^x(1 - p)^{n-x} \\times p^{\\alpha -1} (1 - p)^{\\beta - 1} \\\\ &amp; = p^{x + \\alpha - 1} (1 - p)^{n - x + \\beta - 1} \\end{align} \\] 后验均值 \\[ \\begin{align} E[p ~|~ X] &amp; = \\frac{\\tilde \\alpha}{\\tilde \\alpha + \\tilde \\beta}\\\\ \\\\ &amp; = \\frac{x + \\alpha}{x + \\alpha + n - x + \\beta}\\\\ \\\\ &amp; = \\frac{x + \\alpha}{n + \\alpha + \\beta} \\\\ \\\\ &amp; = \\frac{x}{n} \\times \\frac{n}{n + \\alpha + \\beta} + \\frac{\\alpha}{\\alpha + \\beta} \\times \\frac{\\alpha + \\beta}{n + \\alpha + \\beta} \\\\ \\\\ &amp; = \\mbox{MLE} \\times \\pi + \\mbox{Prior Mean} \\times (1 - \\pi) \\end{align} \\] 后验均值是先验均值与最大似然估计的混合 当 \\(n\\) 变大 \\(\\pi\\) 接近 \\(1\\) 先验作用小 当 \\(n\\) 很小 先验作用大 当数据量够大时 先验概率作用就很小了 当先验概率足够稳定 数据就作用不大了 信任区间 \\(95\\%\\) 信任区间 \\([a, b]\\) 会满足\\(P(p \\in [a, b] ~|~ x) = .95\\) 最高后验密度 (HPD) 区间 7.13 两独立样本t检验 \\(X_1,\\ldots,X_{n_x}\\) 为 iid \\(N(\\mu_x,\\sigma^2)\\) \\(Y_1,\\ldots,Y_{n_y}\\) 为 iid \\(N(\\mu_y, \\sigma^2)\\) \\(\\bar X\\), \\(\\bar Y\\), \\(S_x\\), \\(S_y\\) 为均值与标准差 根据均值与方差的线性组合 有 \\(\\bar Y - \\bar X\\) 也是正态 均值 \\(\\mu_y - \\mu_x\\) 方差 \\(\\sigma^2 (\\frac{1}{n_x} + \\frac{1}{n_y})\\) 混合方差为 \\(S_p^2 = \\{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\\}/(n_x + n_y - 2)\\) 为\\(\\sigma^2\\)的良好估计 该估计为无偏估计 \\[ \\begin{eqnarray*} E[S_p^2] &amp; = &amp; \\frac{(n_x - 1) E[S_x^2] + (n_y - 1) E[S_y^2]}{n_x + n_y - 2}\\\\ &amp; = &amp; \\frac{(n_x - 1)\\sigma^2 + (n_y - 1)\\sigma^2}{n_x + n_y - 2} \\end{eqnarray*} \\] 该估计独立于 \\(\\bar Y - \\bar X\\) 因为方差独立于均值 两个独立的卡方变量之和是自由度之和的卡方值 \\[ \\begin{eqnarray*} (n_x + n_y - 2) S_p^2 / \\sigma^2 &amp; = &amp; (n_x - 1)S_x^2 /\\sigma^2 + (n_y - 1)S_y^2/\\sigma^2 \\\\ \\\\ &amp; = &amp; \\chi^2_{n_x - 1} + \\chi^2_{n_y-1} \\\\ \\\\ &amp; = &amp; \\chi^2_{n_x + n_y - 2} \\end{eqnarray*} \\] 构建统计量 \\[ \\frac{\\frac{\\bar Y - \\bar X - (\\mu_y - \\mu_x)}{\\sigma \\left(\\frac{1}{n_x} + \\frac{1}{n_y}\\right)^{1/2}}}{\\sqrt{\\frac{(n_x + n_y - 2) S_p^2}{(n_x + n_y - 2)\\sigma^2}}} = \\frac{\\bar Y - \\bar X - (\\mu_y - \\mu_x)}{S_p \\left(\\frac{1}{n_x} + \\frac{1}{n_y}\\right)^{1/2}} \\] 该统计量为符合自由度 \\(n_x + n_y - 2\\) 的 \\(t\\) 分布 置信区间 \\[ \\bar Y - \\bar X \\pm t_{n_x + n_y - 2, 1 - \\alpha/2}S_p\\left(\\frac{1}{n_x} + \\frac{1}{n_y}\\right)^{1/2} \\] 方差不等 \\[ \\bar Y - \\bar X \\sim N\\left(\\mu_y - \\mu_x, \\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y}\\right) \\] 统计量 \\[ \\frac{\\bar Y - \\bar X - (\\mu_y - \\mu_x)}{\\left(\\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y}\\right)^{1/2}} \\] 近似于自由度 \\[ \\frac{\\left(S_x^2 / n_x + S_y^2/n_y\\right)^2} {\\left(\\frac{S_x^2}{n_x}\\right)^2 / (n_x - 1) + \\left(\\frac{S_y^2}{n_y}\\right)^2 / (n_y - 1)} \\] 的\\(t\\)分布 7.14 假设检验 使用数据做决定 空假设 \\(H_0\\) 无变化 备择假设 \\(H_a\\) 或大 或小 或不等 真值表 Truth Decide Result \\(H_0\\) \\(H_0\\) Correctly accept null \\(H_0\\) \\(H_a\\) Type I error \\(H_a\\) \\(H_a\\) Correctly reject null \\(H_a\\) \\(H_0\\) Type II error Z检验 Z检验 \\(H_0:\\mu = \\mu_0\\) 与 \\(H_1: \\mu &lt; \\mu_0\\) \\(H_2: \\mu \\neq \\mu_0\\) \\(H_3: \\mu &gt; \\mu_0\\) 检验统计量 \\(TS = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\\) 拒绝空假设条件 \\(TS \\leq -Z_{1 - \\alpha}\\) \\(|TS| \\geq Z_{1 - \\alpha / 2}\\) \\(TS \\geq Z_{1 - \\alpha}\\) 样本数要足够 否则选 \\(t\\) 检验 通过 \\(\\alpha\\) 控制了 Type I error 但没控制 \\(\\beta\\) Type II error 所以结论为没有拒绝 \\(H_0\\) 而不是接受 \\(H_0\\) 拒绝 \\(H_0\\) 的值域为拒绝域 二项分布不易做正态假设可精确计算拒绝域 7.15 P 值 假定没有事发生 出现状况的可能性 先定义分布 然后计算相关统计量 对比常见阈值看数值是否够极端 阈值为达到显著性水平 与p值有区别 p值可设定任意显著性水平 小于就可以拒绝 两尾检验 单尾概率翻倍 独立于假设检验 但常常一起使用 7.16 功效 错误拒绝空假设的概率为功效（power） Power \\(= 1 - \\beta\\) 对 Type II error 的控制 正态分布假设下的推导 \\[ \\begin{align} 1 -\\beta &amp; = P\\left(\\frac{\\bar X - 30}{\\sigma /\\sqrt{n}} &gt; z_{1-\\alpha} ~|~ \\mu = \\mu_a \\right)\\\\ &amp; = P\\left(\\frac{\\bar X - \\mu_a + \\mu_a - 30}{\\sigma /\\sqrt{n}} &gt; z_{1-\\alpha} ~|~ \\mu = \\mu_a \\right)\\\\ \\\\ &amp; = P\\left(\\frac{\\bar X - \\mu_a}{\\sigma /\\sqrt{n}} &gt; z_{1-\\alpha} - \\frac{\\mu_a - 30}{\\sigma /\\sqrt{n}} ~|~ \\mu = \\mu_a \\right)\\\\ \\\\ &amp; = P\\left(Z &gt; z_{1-\\alpha} - \\frac{\\mu_a - 30}{\\sigma /\\sqrt{n}} ~|~ \\mu = \\mu_a \\right)\\\\ \\\\ \\end{align} \\] sigma &lt;- 10; mu_0 = 0; mu_a = 2; n &lt;- 100; alpha = .05 plot(c(-3, 6),c(0, dnorm(0)), type = &quot;n&quot;, frame = F, xlab = &quot;Z value&quot;, ylab = &quot;&quot;) xvals &lt;- seq(-3, 6, length = 1000) lines(xvals, dnorm(xvals), type = &quot;l&quot;, lwd = 3) lines(xvals, dnorm(xvals, mean = sqrt(n) * (mu_a - mu_0) / sigma), lwd =3) abline(v = qnorm(1 - alpha)) - 计算步骤 - 考虑 \\(H_0 : \\mu = \\mu_0\\) 与 \\(H_a : \\mu &gt; \\mu_0\\) 且在\\(H_a\\)下 \\(\\mu = \\mu_a\\) - 在 \\(H_0\\) 下统计量 \\(Z = \\frac{\\sqrt{n}(\\bar X - \\mu_0)}{\\sigma}\\) 符合 \\(N(0, 1)\\) - 在\\(H_a\\)下 \\(Z\\) 是 \\(N\\left( \\frac{\\sqrt{n}(\\mu_a - \\mu_0)}{\\sigma}, 1\\right)\\) - 如果 \\(Z &gt; Z_{1-\\alpha}\\) 拒绝空假设 也就是给定条件下功效不够 - 当检验 \\(H_a : \\mu &gt; \\mu_0\\), 如果功效为 \\(1 - \\beta\\) 那么 \\(1 - \\beta = P\\left(Z &gt; z_{1-\\alpha} - \\frac{\\mu_a - \\mu_0}{\\sigma /\\sqrt{n}} ~|~ \\mu = \\mu_a \\right) = P(Z &gt; z_{\\beta})\\) 也就是 \\(z_{1-\\alpha} - \\frac{\\sqrt{n}(\\mu_a - \\mu_0)}{\\sigma} = z_{\\beta}\\) - \\(\\mu_a\\), \\(\\sigma\\), \\(n\\), \\(\\beta\\), \\(\\mu_0\\), \\(\\alpha\\) 给定五个可解出剩余的 - 两尾检验考虑 \\(\\alpha / 2\\) - 功效在 \\(\\alpha\\) 提高 单尾检验功效高于两尾 \\(\\mu_1\\) 距离 \\(\\mu_0\\) 远功效大 样本数提高功效高 - 计算功效不需要特定样本 只需要指定 \\(\\frac{\\mu_a - \\mu_0}{\\sigma}\\) 也就是有效样本大小 无单位 - R 中使用 power.t.test 来计算 \\(t\\) 检验功效相关参数 指定多数求一个 7.17 多重比较 多次进行比较会导致错误率与校正出现问题 False positive rate 错误结果是显著的比率 \\(\\alpha\\) 样本数增大错误增加 Family wise error rate (FWER) 所有比较中至少一个假阳性比率 Bonferroni correction 假设你进行m次测试 控制 \\(\\alpha\\) 在某水平 计算所有测试的 \\(p\\) 值 将 \\(\\alpha\\) 设为 $ /m$ 所有测试都在这个置信度下进行 容易计算 过于保守 False discovery rate (FDR) 声称显著是错误的概率 \\(m\\) 次测试 水平 \\(\\alpha\\) 计算 \\(p\\) 值 排序 \\(P_{(i)} \\leq \\alpha \\times \\frac{i}{m}\\) 为显著 相对容易计算 不保守 允许一定的假阳性 调节p值 \\(P_i^{fwer} = \\max{m \\times P_i,1}\\) 类似FWER处理 \\(\\alpha\\) 的方式处理 \\(p\\) 按照正常 \\(\\alpha\\) 检测 一般情况对 \\(p\\) 值用 bonferroni/BH矫正就够了 对比间依赖强烈考虑 method=“BY” 多重比较从原理到应用 从实用角度分类 适合常见科研实验结果处理 7.18 重采样推断 jackknife 用来无偏估计偏差与标准误 每次估计删掉一个数据 \\(\\bar \\theta = \\frac{1}{n}\\sum_{i=1}^n \\hat \\theta_{i}\\) 偏差 \\((n - 1) \\left(\\bar \\theta - \\hat \\theta\\right)\\) 标准误 \\(\\left[\\frac{n-1}{n}\\sum_{i=1}^n (\\hat \\theta_i - \\bar\\theta )^2\\right]^{1/2}\\) 可用来估计分位数 是bootstrap的线性逼近 但性质不好 假观察量角度理解jackknife \\(\\mbox{Pseudo Obs} = n \\hat \\theta - (n - 1) \\hat \\theta_{i}\\) 生成原数据集 bootstrap 构建置信区间与求标准误 假定采样分布是总体分布 重采样估计统计量 有放回的重采样 \\(B\\) 次 \\(N\\) 个样本 得到估计统计量的一个分布 直接计算置信区间 非参方法 偏差小 进阶指南 置换检验 分组对比时取消原分组随机分组 重复进行 记录分组差异 对比原参数与置换后参数差异进行推断 "],
["section-8.html", "Notes 8 回归模型 8.1 导论 8.2 术语 8.3 回归线的最小二乘回归 8.4 统计线性回归模型 8.5 残差 8.6 回归推断 8.7 多元回归 8.8 模型诊断与选择 8.9 广义线性模型 8.10 二元响应 8.11 计数或速率响应 8.12 分段平滑", " Notes 8 回归模型 8.1 导论 Francis Galton 1885年用父母身高预测子女身高的案例 考虑单变量的数据代表：最小二乘值 最小二乘值物理意义为质心 最小二乘统计学意义是平均值 可用不等式解 也可用求导方法解 \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\mu)^2 &amp; = \\ \\sum_{i=1}^n (Y_i - \\bar Y + \\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 \\sum_{i=1}^n (Y_i - \\bar Y) (\\bar Y - \\mu) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 (\\bar Y - \\mu) \\sum_{i=1}^n (Y_i - \\bar Y) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\ 2 (\\bar Y - \\mu) (\\sum_{i=1}^n Y_i - n \\bar Y) +\\ \\sum_{i=1}^n (\\bar Y - \\mu)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\bar Y)^2 + \\sum_{i=1}^n (\\bar Y - \\mu)^2\\\\ &amp; \\geq \\sum_{i=1}^n (Y_i - \\bar Y)^2 \\ \\end{align} \\] 通过原点的回归 最小化\\(\\sum_{i=1}^n (Y_i - X_i \\beta)^2\\) 两变量关系用回归线解释 8.2 术语 \\(X_1, X_2, \\ldots, X_n\\) 表示 \\(n\\) 个数据点 \\(Y_1, \\ldots , Y_n\\) 表示另外 \\(n\\) 个数据点 用希腊字母表示不知道的东西 如 \\(\\mu\\) 大写字母表示概念值 小写字母表示真实值 如 \\(P(X_i &gt; x)\\) \\(\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i\\) 表示均值 数据的中心趋向 \\(\\tilde X_i = X_i - \\bar X\\) 表示对数据中心化 均值为0 均值为数据的最小二乘估计 \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^n X_i^2 - n \\bar X ^ 2 \\right)\\) 表示方差 \\(S\\) 为标准差 数据的离散程度 \\(X_i / s\\) 表示数据缩放 方差为1 \\(Z_i = \\frac{X_i - \\bar X}{s}\\) 表示数据的标准化 先中心化再标准化 \\(Cov(X, Y) = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X) (Y_i - \\bar Y)= \\frac{1}{n-1}\\left( \\sum_{i=1}^n X_i Y_i - n \\bar X \\bar Y\\right)\\) 表示协方差 \\(Cor(X, Y) = \\frac{Cov(X, Y)}{S_x S_y}\\) 表示相关性 \\(Cor(X, Y) = Cor(Y, X)\\) \\(-1 \\leq Cor(X, Y) \\leq 1\\) \\(Cor(X, Y)\\) 度量线性关系强度 \\(Cor(X, Y) = 0\\) 表示无线性关系 8.3 回归线的最小二乘回归 用最小二乘法寻找回归线 最小化 \\(\\sum_{i=1}^n \\{Y_i - (\\beta_0 + \\beta_1 X_i)\\}^2\\) 如果定义 \\(\\mu_i = \\beta_0\\) \\(\\hat \\beta_0 = \\bar Y\\) 不考虑其他变量 \\(Y\\) 的均值就是最小二乘估计 如果定义 \\(\\mu_i = X_i \\beta_1\\) \\(\\hat \\beta_1 = \\frac{\\sum_{i=1^n} Y_i X_i}{\\sum_{i=1}^n X_i^2}\\) 如果考虑过原点线的回归 斜率如上 如果考虑 \\(\\mu_i = \\beta_0 + \\beta_1 X_i\\) \\[\\begin{align} \\ \\sum_{i=1}^n (Y_i - \\hat \\mu_i) (\\hat \\mu_i - \\mu_i) = &amp; \\sum_{i=1}^n (Y_i - \\hat\\beta_0 - \\hat\\beta_1 X_i) (\\hat \\beta_0 + \\hat \\beta_1 X_i - \\beta_0 - \\beta_1 X_i) \\\\ = &amp; (\\hat \\beta_0 - \\beta_0) \\sum_{i=1}^n (Y_i - \\hat\\beta_0 - \\hat \\beta_1 X_i) + (\\beta_1 - \\beta_1)\\sum_{i=1}^n (Y_i - \\hat\\beta_0 - \\hat \\beta_1 X_i)X_i\\\\ \\end{align} \\] 解为\\(\\hat \\beta_1 = Cor(Y, X) \\frac{Sd(Y)}{Sd(X)} ~~~ \\hat \\beta_0 = \\bar Y - \\hat \\beta_1 \\bar X\\) 如果标准化数据 \\(\\{ \\frac{X_i - \\bar X}{Sd(X)}, \\frac{Y_i - \\bar Y}{Sd(Y)}\\}\\) 解为\\(Cor(Y, X)\\) 回归是因变量向自己均值回归与向自变量相关回归的平衡 8.4 统计线性回归模型 最小二乘是一种估计方法，做推断需要模型 建立线性回归的概率模型\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_{i}\\) \\(\\epsilon_{i}\\) 为 iid \\(N(0, \\sigma^2)\\) \\(E[Y_i ~|~ X_i = x_i] = \\mu_i = \\beta_0 + \\beta_1 x_i\\) \\(Var(Y_i ~|~ X_i = x_i) = \\sigma^2\\) 对\\(N(\\mu_i, \\sigma^2)\\)独立变量 \\(Y\\) 进行极大似然估计 \\({\\cal L}(\\beta, \\sigma) = \\prod_{i=1}^n \\left\\{(2 \\pi \\sigma^2)^{-1/2}\\exp\\left(-\\frac{1}{2\\sigma^2}(y_i - \\mu_i)^2 \\right) \\right\\}\\) 取对数有 \\(-2 \\log\\{ {\\cal L}(\\beta, \\sigma) \\} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - \\mu_i)^2 + n\\log(\\sigma^2)\\) 最小二乘估计就是极大似然估计 \\(\\hat \\beta_1 = Cor(Y, X) \\frac{Sd(Y)}{Sd(X)} ~~~ \\hat \\beta_0 = \\bar Y - \\hat \\beta_1 \\bar X\\) 截距是自变量为0时 \\(Y\\) 的期望 斜率是自变量变化一个单位对 \\(Y\\) 的影响 8.5 残差 模型 \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) 预测值 \\(\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\\) \\(e_i = Y_i - \\hat Y_i\\) 观察数据与回归线的垂直距离 最小二乘估计最小化残差 \\(\\sum_{i=1}^n e_i^2\\) 残差 \\(e_i\\) 可看作 \\(\\epsilon_i\\) 的估计 可证 \\(E[e_i] = 0\\) 模型中考虑截距 \\(\\sum_{i=1}^n e_i = 0\\) 考虑自变量 \\(\\sum_{i=1}^n e_i X_i = 0\\) 残差可用来评价模型效果 残差波动不同于模型波动 残差波动 \\(\\sigma^2\\) 的极大似然估计为 \\(\\frac{1}{n}\\sum_{i=1}^n e_i^2\\) \\(\\hat \\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2\\) 为无偏估计 \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\bar Y)^2 &amp; = \\sum_{i=1}^n (Y_i - \\hat Y_i + \\hat Y_i - \\bar Y)^2 \\\\ &amp; = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 + 2 \\sum_{i=1}^n (Y_i - \\hat Y_i)(\\hat Y_i - \\bar Y) + \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2 \\\\ \\end{align} \\] 其中 \\((Y_i - \\hat Y_i) = \\{Y_i - (\\bar Y - \\hat \\beta_1 \\bar X) - \\hat \\beta_1 X_i\\} = (Y_i - \\bar Y) - \\hat \\beta_1 (X_i - \\bar X)\\) \\((\\hat Y_i - \\bar Y) = (\\bar Y - \\hat \\beta_1 \\bar X - \\hat \\beta_1 X_i - \\bar Y )= \\hat \\beta_1 (X_i - \\bar X)\\) 有\\(\\sum_{i=1}^n (Y_i - \\hat Y_i)(\\hat Y_i - \\bar Y) = \\sum_{i=1}^n \\{(Y_i - \\bar Y) - \\hat \\beta_1 (X_i - \\bar X))\\}\\{\\hat \\beta_1 (X_i - \\bar X)\\}=\\hat \\beta_1 \\sum_{i=1}^n (Y_i - \\bar Y)(X_i - \\bar X) -\\hat\\beta_1^2\\sum_{i=1}^n (X_i - \\bar X)^2= \\hat \\beta_1^2 \\sum_{i=1}^n (X_i - \\bar X)^2-\\hat\\beta_1^2\\sum_{i=1}^n (X_i - \\bar X)^2 = 0\\) 综上 \\(\\sum_{i=1}^n (Y_i - \\bar Y)^2 = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 + \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2\\) 有 Total Variation = Residual Variation + Regression Variation 模型解释部分\\(R^2 = \\frac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2} = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}\\) 已知 \\((\\hat Y_i - \\bar Y) = \\hat \\beta_1 (X_i - \\bar X)\\) \\(\\hat \\beta_1 = Cor(Y, X)\\frac{Sd(Y)}{Sd(X)}\\) 有 \\(R^2 = \\frac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}= \\hat \\beta_1^2 \\frac{\\sum_{i=1}^n(X_i - \\bar X)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}= Cor(Y, X)^2\\) \\(R^2\\) 实际上是相关性 \\(r\\) 的平方 &lt;- 线性模型的可解释性 \\(R^2\\) 会伴随样本数增加而增加 会因删除异常值而增加 data(anscombe);example(anscombe) 8.6 回归推断 \\(\\frac{\\hat \\theta - \\theta}{\\hat \\sigma_{\\hat \\theta}}\\) 总符合正态分布或\\(t\\)分布 假设检验 \\(H_0 : \\theta = \\theta_0\\) 与 \\(H_a : \\theta &gt;, &lt;, \\neq \\theta_0\\) 置信区间 \\(\\theta\\) 通过 \\(\\hat \\theta \\pm Q_{1-\\alpha/2} \\hat \\sigma_{\\hat \\theta}\\) 构建 \\[ \\begin{align} Var(\\hat \\beta_1) &amp; = Var\\left(\\frac{\\sum_{i=1}^n (Y_i - \\bar Y) (X_i - \\bar X)}{\\sum_{i=1}^n (X_i - \\bar X)^2}\\right) \\\\ &amp; = \\frac{Var\\left(\\sum_{i=1}^n Y_i (X_i - \\bar X) \\right) }{\\left(\\sum_{i=1}^n (X_i - \\bar X)^2 \\right)^2} \\\\ &amp; = \\frac{\\sum_{i=1}^n \\sigma^2(X_i - \\bar X)^2}{\\left(\\sum_{i=1}^n (X_i - \\bar X)^2 \\right)^2} \\\\ &amp; = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar X)^2} \\\\ \\end{align} \\] \\(\\sigma_{\\hat \\beta_1}^2 = Var(\\hat \\beta_1) = \\sigma^2 / \\sum_{i=1}^n (X_i - \\bar X)^2\\) \\(\\sigma_{\\hat \\beta_0}^2 = Var(\\hat \\beta_0) = \\left(\\frac{1}{n} + \\frac{\\bar X^2}{\\sum_{i=1}^n (X_i - \\bar X)^2 }\\right)\\sigma^2\\) 这样 \\(\\frac{\\hat \\beta_j - \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\) 遵守自由度为\\(n-2\\)的\\(t\\)分布或正态分布 在\\(x_0\\) 回归线的标准误 \\(\\hat \\sigma\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar X)^2}{\\sum_{i=1}^n (X_i - \\bar X)^2}}\\) 在\\(x_0\\) 预测值的标准误 \\(\\hat \\sigma\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar X)^2}{\\sum_{i=1}^n (X_i - \\bar X)^2}}\\) CI代表回归线在特定\\(x\\)处的变动 PI代表预测值在此处的变动 前者在回归线固定时不变 后者还要考虑预测值围绕回归线的变动 The prediction interval is the range in which future observation can be thought most likely to occur, whereas the confidence interval is where the mean of future observation is most likely to reside. From here 8.7 多元回归 线性模型 \\(Y_i = \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_{p} X_{pi} + \\epsilon_{i} = \\sum_{k=1}^p X_{ik} \\beta_j + \\epsilon_{i}\\) 最小化 \\(\\sum_{i=1}^n \\left(Y_i - \\sum_{k=1}^p X_{ki} \\beta_j\\right)^2\\) 最小二乘估计也是误差正态化的极大似然估计 最小二乘估计等价于 \\(\\sum_{i=1}^n (Y_i - X_{1i}\\hat \\beta_1 - \\ldots - X_{ip}\\hat \\beta_p) X_k = 0\\) 本质上使其他参数固定解出一个 然后逐级代入 最后全部解出参数值 参考线性代数 参数代表固定其他参数后变动一个单位引发的变化 方差估计 \\(\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^n e_i ^2\\) 参数标准误\\(\\hat \\sigma_{\\hat \\beta_k}\\) \\(\\frac{\\hat \\beta_k - \\beta_k}{\\hat \\sigma_{\\hat \\beta_k}}\\) 符合自由度 \\(n-p\\) 的 \\(T\\) 分布 多元模型中加入变量会导致原有变量的参数估计发生变化 甚至方向相反 一般是由于加入变量与原有变量存在共相关 导致两者参数估计都不准 n &lt;- 100; x2 &lt;- 1 : n; x1 &lt;- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01) summary(lm(y ~ x1))$coef summary(lm(y ~ x1 + x2))$coef R 会自动检测并消除变量生成的变量 如上面 x2 中需要加入 runif(n,-.1,.1) 才能得到结果 多元模型中包括分类变量考虑加入虚拟变量 \\(Y_i = \\beta_0 + X_{i1} \\beta_1 + \\epsilon_{i}\\) 属于该分类时 \\(E[Y_i] = \\beta_0 + \\beta_1\\) 否则为\\(E[Y_i] = \\beta_0\\) 分类变量截距有意义 代表其中一个分类 等同于其他分类与该分类进行 t 检验 如果模型中去掉截距 等同于所有分类与零进行 t 检验 参数系数为均值差 可用 relevel(data,'name') 来指定比对对象 两变量均值差的标准误通过 \\(Var(\\hat \\beta_B - \\hat \\beta_C) = Var(\\hat \\beta_B) + Var(\\hat \\beta_C) - 2 Cov(\\hat \\beta_B, \\hat \\beta_C)\\) 来计算进行推断 交互作用 \\(E[Y_i | X_{1i}=x_1, X_{2i}=x_2] = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\beta_3 x_{1}x_{2}\\) 中交互作用参数实际表示 \\(E[Y_i | X_{1i}=x_1+1, X_{2i}=x_2+1]-E[Y_i | X_{1i}=x_1, X_{2i}=x_2+1]-E[Y_i | X_{1i}=x_1+1, X_{2i}=x_2]-E[Y_i | X_{1i}=x_1, X_{2i}=x_2] =\\beta_3\\) 各交互参数变化一单位响应变化 多元回归的参数解释需要考虑清楚变量类型与交互作用 多元回归中变量与响应 变量与变量间的相关性要全盘考虑 通过模拟观察决定 8.8 模型诊断与选择 通过残差诊断 最小二乘决定均值为零 方差通过 \\(\\hat \\sigma^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-p}\\) 进行无偏估计 异常值判断 对回归关系包括系数与其标准误的影响 残差的分布检验等 ?influence.measures There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don’t know. But there are also unknown unknowns. There are things we don’t know we don’t know. Donald Rumsfeld 随机化有助于平衡未知变量 杠杆点 加入前后与回归线距离差的比值 参数方差膨胀 共相关或随机相关 vif来检验 协变量在欠拟合下有偏 协变量的选择需要专业知识与经验 8.9 广义线性模型 Nelder 与 Wedderburn 1972年提出 响应是指数家族模型 模型组成部分是线性的 线性预测变量与响应通过连接函数联系 线性模型 \\(Y_i \\sim N(\\mu_i, \\sigma^2)\\) \\(\\eta_i = \\sum_{k=1}^p X_{ik} \\beta_k\\) \\(g(\\mu) = \\eta\\) 似然模型为 \\(Y_i = \\sum_{k=1}^p X_{ik} \\beta_k + \\epsilon_{i}\\) \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) logistic 模型 \\(Y_i \\sim Bernoulli(\\mu_i)\\) \\(\\eta_i = \\sum_{k=1}^p X_{ik} \\beta_k\\) \\(g(\\mu) = \\eta = \\log\\left( \\frac{\\mu}{1 - \\mu}\\right)\\) \\(g\\)为logit函数 似然函数为 \\(\\prod_{i=1}^n \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i} = \\exp\\left(\\sum_{i=1}^n y_i \\eta_i \\right) \\prod_{i=1}^n (1 + \\eta_i)^{-1}\\) 泊松模型 \\(Y_i \\sim Poisson(\\mu_i)\\) \\(\\eta_i = \\sum_{k=1}^p X_{ik} \\beta_k\\) \\(g(\\mu) = \\eta = \\log(\\mu)\\) 似然函数为 \\(\\prod_{i=1}^n (y_i !)^{-1} \\mu_i^{y_i}e^{-\\mu_i}\\propto \\exp\\left(\\sum_{i=1}^n y_i \\eta_i - \\sum_{i=1}^n \\mu_i\\right)\\) 似然函数与数据的联系 \\(\\sum_{i=1}^n y_i \\eta_i = \\sum_{i=1}^n y_i\\sum_{k=1}^p X_{ik} \\beta_k = \\sum_{k=1}^p \\beta_k\\sum_{i=1}^n X_{ik} y_i\\) 只有\\(\\sum_{i=1}^n X_{ik} y_i\\) 极大似然估计的解 \\(0=\\sum_{i=1}^n \\frac{(Y_i - \\mu_i)}{Var(Y_i)}W_i\\) \\(W_i\\)是连接函数的反函数的微分 响应的方差中线性模型 \\(Var(Y_i) = \\sigma^2\\) 是常数 logistic 模型 \\(Var(Y_i) = \\mu_i (1 - \\mu_i)\\) 泊松模型 \\(Var(Y_i) = \\mu_i\\) 可通过对模型方差增加调谐参数 \\(\\phi\\) 使模型更灵活 quasi-likelihood 模型求解为 \\(\\hat \\beta_k\\) 及可能的 \\(\\hat \\phi\\) 线性预测变量关系 \\(\\hat \\eta = \\sum_{k=1}^p X_k \\hat \\beta_k\\) 平均响应 \\(\\hat \\mu = g^{-1}(\\hat \\eta)\\) 系数解释 \\(g(E[Y | X_k = x_k + 1, X_{\\sim k} = x_{\\sim k}]) - g(E[Y | X_k = x_k, X_{\\sim k}=x_{\\sim k}]) = \\beta_k\\) 8.10 二元响应 \\(\\log\\left(\\frac{\\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\\rm{Pr}(RW_i | RS_i, b_0, b_1)}\\right) = b_0 + b_1 RS_i\\) \\(b_0\\) 预测变量为零时胜率对数 \\(b_1\\) 预测变量变化一个单位胜率的改变对数 \\(\\exp(b_1)\\) 预测变量变化一个单位胜率的改变 8.11 计数或速率响应 \\(\\log\\left(E[NH_i | JD_i, b_0, b_1]\\right) = b_0 + b_1 JD_i\\) \\(e^{E[\\log(Y)]}\\) \\(Y\\) 的几何平均值 \\(e^{\\beta_0}\\) 第零天的几何平均值 \\(e^{\\beta_1}\\) 每天相对增加或减少的几何平均值 通过设置 offset 可用来估计增长率 注意方差膨胀与零膨胀问题 8.12 分段平滑 可用线性回归拟合曲线 原理是分段拟合连接 断点平滑可用二次项 分段项可看作基进行组合 "],
["section-9.html", "Notes 9 机器学习实战 9.1 研究设计 9.2 错误率 9.3 ROC 曲线 9.4 交叉检验 9.5 caret 包 9.6 数据分割 9.7 训练选项 9.8 预测变量作图 9.9 数据预处理 9.10 协变量生成 9.11 线性回归&amp;多元线性回归 9.12 树 9.13 Bagging 9.14 radom forest 9.15 boosting 9.16 其他预测算法 9.17 模型联合 9.18 无监督预测 9.19 预测", " Notes 9 机器学习实战 问题 -&gt; 数据 -&gt; 特征 -&gt; 算法 -&gt; 参数 -&gt; 评价 The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. John Tukey 数据质量优先于模型 不要自动特征选择 算法的可扩展性与计算性能要考虑 数据过拟合问题 数据总是由信号与噪音组成 但会被算法无差别对待 数据要与问题相关 低相关度的组合可能产生高相关度 9.1 研究设计 定义错误率 将数据分割为训练集 预测集 验证集 在训练集上使用交叉检验选择特征与预测算法 在预测集或验证集上使用一次数据 预测效果起码要优于瞎猜 避免使用小样本 比例为 60% 训练集 20% 预测集 20% 验证集 或 60% 训练集 40% 预测集 或小样本交叉检验 注意数据结构 时序分析要对数据分段采样 9.2 错误率 真阳性 真的是对的 TP 假阳性 真的是错的 FP Type I 真阴性 假的是错的 TN 假阴性 假的是对的 FN Type II 灵敏度 TP/(TP+FP) 特异性 TN/(TN+FN) 均方差 MSE \\(\\frac{1}{n} \\sum_{i=1}^n (Prediction_i - Truth_i)^2\\) 均方误 RMSE \\(\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(Prediction_i - Truth_i)^2}\\) 中位差 Median absolute deviation 准确性 (TP+TN)/(TP+FP+TN+FP) 一致性 kappa值 9.3 ROC 曲线 分类问题寻找判别阈值 满足一定TP下最小FP的模型 FP v.s.TP 作图 AUC 曲线下面积表示选择标准 一般超过80% 对角线是随机猜的结果 9.4 交叉检验 训练集上的操作 训练集上再分为训练集与测试集 在测试集上评价 重复并平均化测试集错误 用来进行变量 模型 参数选择 随机 分组 留一 分组多方差大 分组少有偏差 有放回的为bootstrap 不建议用 9.5 caret 包 数据清洗 预处理 数据分割 createDataPartition 数据比例 重采样 产生时间片段 训练检验整合函数 train predict 模型对比 算法整合为选项 线性判别 回归 朴素贝叶斯 支持向量机 分类与回归树 随机森林 Boosting 等 9.6 数据分割 train &lt;- createDataPartition(y=spam$type,p=0.75, list=FALSE) 数据三一分 得到index folds &lt;- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE) 数据分10份 返回每一份列表 folds &lt;- createResample(y=spam$type,times=10,list=TRUE) 数据bootstrap重采样 返回每一份列表 folds &lt;- createTimeSlices(y=tme,initialWindow=20,horizon=10) 时序数据重采样 产生20为窗口时序片段的训练集与预测集 9.7 训练选项 args(train.default) 通过 method 控制算法 metric 控制算法评价 trainControl 控制训练方法 trainControl中 method选择模型选择方法 如bootstrap 交叉检验 留一法 number 控制次数 repeats 控制重采样次数 seed 控制可重复性 总体设置一个 具体每一次用列表设置控制具体过程 特别是并行模型 9.8 预测变量作图 featurePlot ggplot2 9.9 数据预处理 train 中的 preProcess=c(&quot;center&quot;,&quot;scale&quot;) 标准化 spatialSign 该转化可提高计算效率 有偏 preProcess(training[,-58],method=c(&quot;BoxCox&quot;)) 正态化转化 method=&quot;knnImpute&quot; 用最小邻近法填补缺失值 nearZeroVar 去除零方差变量 findCorrelation 去除相关变量 findLinearCombos 去除线性组合变量 classDist 测定分类变量的距离 生成新变量 测试集也要预处理 9.10 协变量生成 原始数据提取特征 提取特征后生成新变量 因子变量要转为虚拟变量 样条基变量 splines 包中的 bs 数据压缩 preProcess 中 method 设置为 pca pcaComp 指定主成分个数 9.11 线性回归&amp;多元线性回归 \\(ED_i = b_0 + b_1 WT_i + e_i\\) 基本模型 参见前面回归部分 9.12 树 迭代分割变量 在最大化预测时分割 评估分支的同质性 多个树的预测更好 优点 容易解释应用 可用在神经网络上 缺点 不容易交叉验证 不确定性不宜估计 结果可能变化 -算法 先在一个组里用所有的变量计算 寻找最容易分离结果的变量 把数据按照该变量节点分为两组 在每一个组中寻找最好的分离变量 迭代直到过程结束 节点纯度用 Gini 系数或 交叉墒来衡量 rattle 包的 fancyRpartPlot 出图漂亮 可用来处理非线性模型与变量选择 9.13 Bagging 重采样 重新计算预测值 平均或投票给出结果 减少方差 偏差类似 适用于非线性过程 bagged trees 重采样 重建树 结果重评价 更稳健 效果不如RF Bagged loess 可用来处理细节 9.14 radom forest bootstrap采样 每一个节点bootstrap选取变量 多棵树投票 准确度高 速度慢 不好解释 容易过拟合 9.15 boosting 弱预测变量加权后构建强预测变量 从一组预测变量开始 添加有惩罚项的预测变量来训练模型 以降低训练集误差为目的 通用方法 9.16 其他预测算法 参考统计学习导论笔记 9.17 模型联合 通过平均与投票结合模型 联合分类器提高准确率 caretEnsemble 包 案例 广义加性模型 library(ISLR); data(Wage); library(ggplot2); library(caret); Wage &lt;- subset(Wage,select=-c(logwage)) # Create a building data set and validation set inBuild &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE) validation &lt;- Wage[-inBuild,]; buildData &lt;- Wage[inBuild,] inTrain &lt;- createDataPartition(y=buildData$wage,p=0.7, list=FALSE) training &lt;- buildData[inTrain,]; testing &lt;- buildData[-inTrain,] mod1 &lt;- train(wage ~.,method=&quot;glm&quot;,data=training) mod2 &lt;- train(wage ~.,method=&quot;rf&quot;,data=training,trControl = trainControl(method=&quot;cv&quot;),number=3) pred1 &lt;- predict(mod1,testing); pred2 &lt;- predict(mod2,testing) qplot(pred1,pred2,colour=wage,data=testing) predDF &lt;- data.frame(pred1,pred2,wage=testing$wage) combModFit &lt;- train(wage ~.,method=&quot;gam&quot;,data=predDF) combPred &lt;- predict(combModFit,predDF) sqrt(sum((pred1-testing$wage)^2)) sqrt(sum((pred2-testing$wage)^2)) sqrt(sum((combPred-testing$wage)^2)) 9.18 无监督预测 先聚类 后预测 clue 包 cl_predict 函数 推荐系统 9.19 预测 时序数据 包含趋势 季节变化 循环 效应分解 decompose window 窗口 ma 平滑 ets 指数平滑 forecast 预测 空间数据同样有这种问题 临近依赖 地域效应 quantmod 包 或 quandl 包处理金融数据 外推要谨慎 "],
["section-10.html", "Notes 10 开发数据产品 10.1 shiny 10.2 rCharts 10.3 GoogleVis 10.4 Slidify 10.5 Rpresentation 10.6 yhat 10.7 R 包开发 10.8 R 中方法与类型", " Notes 10 开发数据产品 10.1 shiny 源自 R-studio 动态网络应用 入门版 OpenCPU 高级版 Manipulate install.packages(&quot;shiny&quot;);libray(shiny) ui.R 控制外观 sever.R 控制计算 runApp() 启动应用 sever.R 中 shinyServer 之前的代码只在启动应用时执行一次 适合读入数据 shinyServer(function(input, output){ 之内的非互动函数只被每个用户执行一次 Render* 为互动函数 数值改变就执行一次 runApp(display.mode='showcase') 可用来同时高亮显示执行代码 reactive 用来加速互动函数外的信息交换 actionButton 用来一次提交输入数据 if (input$goButton == 1){ Conditional statements } 用来定义条件语句 cat browser() 调试 fluidRow 产生表格 10.2 rCharts 主页 动态交互可视化工具 require(devtools);install_github('rCharts', 'ramnathv') 10.3 GoogleVis 主页 R 代码产生图表 生成html install.packages('googleVis');library(googleVis) 教程 10.4 Slidify 主页 html5 幻灯片 install.packages(&quot;devtools&quot;);library(devtools);install_github('slidify', 'ramnathv');install_github('slidifyLibraries', 'ramnathv');library(slidify) author(&quot;yufree&quot;) YAML 配置幻灯片结构 ## 幻灯片开始 --- 加空行表结束 .class #id 自定义css文件id slidify(&quot;index.Rmd&quot;) 生成 browseURL(&quot;index.html&quot;) 观看 publish_github(user, repo) github发布 10.5 Rpresentation 源自 R-studio 轻量级幻灯片 教程 10.6 yhat 主页 本地提交算法或模型 生成可调用API 支持R与python 10.7 R 包开发 DESCRIPTION 指明包内容 Package 包名字 Title 全名 Description 一句话描述 Version 版本号 Author 作者 Maintainer 维护者 License 许可协议 Depends 依赖 Suggests 建议 Date 发布日期 YYYY-MM-DD 格式 URL 项目主页 R 源码 Documentation 文档 Rd文件 NAMESPACE 关键词 输入输出的函数及类型 R CMD build/check newpackage 构建 检查包 roxygen2 源文件注释文档 10.8 R 中方法与类型 R 面向对象编程 对象用setClass指定类型 用setMethod指定处理类型的方法 对象一般指新的数据类型 S3函数对象不算严格 generic处理对象 开放 没有指定类型就用通用方法 S4函数对象定义严格 只处理指定类型对象 不可直接调用方法 针对性强 stats4 有很多针对性的极大似然估计的对象定义与方法 "],
["section-11.html", "Notes 11 统计学习导论 11.1 导论 11.2 统计学习 11.3 线性回归 11.4 分类 11.5 重采样技术 11.6 线性模型选择与正则化 11.7 非线性 11.8 决策树 11.9 支持向量机 11.10 无监督学习", " Notes 11 统计学习导论 11.1 导论 11.1.1 统计学习概论 统计学习：理解数据的工具集 监督学习：有因变量，根据自变量预测估计因变量 非监督学习：无因变量，探索自变量间的关系与结构 11.1.2 统计学习简史 19世纪初，Legendre 与 Gauss 发表了最小二乘法的论文，该方法首先应用在天文学领域 1936年，Fisher 提出线性判别分析来解决定性分析问题 1940s，logistic回归提出 1970s，Nelder 与 Wedderburn 提出广义线性模型，将线性回归与logistic回归统一到一个体系 1980s，计算机技术进步，非线性问题开始得到解决 Breiman，Friedman，Olshen 与 Stone 提出回归树与聚类，提供交叉检验方法 1986年，Hastie 与 Tibshirani 提出广义加性模型，将广义线性模型与一些非线性模型同一到一个体系 伴随软件，机器学习与其他理论的发展，统计学习作为统计学子学科快速发展 11.2 统计学习 11.2.1 统计学习定义 \\(Y = f(X) + \\epsilon\\) 统计学习本质上是在寻找最合适的f来进行预测与推断 11.2.2 预测 \\(\\hat Y = \\hat f(X)\\)，\\(\\hat f(X)\\) 通常看作黑箱 \\(\\hat Y\\)预测\\(Y\\)需要考虑两部分误差：可约误差与不可约误差 可约误差指\\(\\hat f\\)推断\\(f\\)上的偏差 不可约误差指由\\(\\epsilon\\)引入的误差 误差的期望 \\(E(Y - \\hat Y)^2 = [f(x) - \\hat f(x)]^2 + Var(\\epsilon)\\) (证明用到\\(E(Y)\\)) 11.2.3 推断 关注X与Y的关系，\\(\\hat f(X)\\) 通常有明确的形式 自变量因变量是否相关 如何相关 关系的数学描述 11.2.4 估计f 使用训练集与验证集 参数方法与非参数方法 模型的欠拟合与过拟合 权衡模型的准确性（预测）与可解释性（推断） 模型的奥卡姆剃刀与黑箱 11.2.5 评价模型 11.2.5.1 拟合质量测量 训练集均方误 \\(MSE_{Tr} = Ave_{i \\in Tr}[y_{i} − \\hat f(x_i)]^2\\) 测试集均方误 \\(MSE_{Te} = Ave_{i \\in Te}[y_{i} − \\hat f(x_i)]^2\\) 测试集均方误源于训练集拟合模型的方差，误差项\\(\\epsilon\\)的方差及模型误差的平方三部分 11.2.5.2 聚类评价 错误率 \\(Err_{Te} = Ave_{i \\in Te}I[y_i \\neq \\hat C(x_i)]\\) 贝叶斯分类器：错误率最小的分类器，使x属于某个分类的概率最大 k临近值聚类：距离最小的k个为一类所产生的分类器 11.3 线性回归 11.3.1 简单线性回归 \\(Y \\approx \\beta_0 + \\beta_1 X\\) 用最小二乘法估计\\(\\beta_0\\)与\\(\\beta_1\\)得到估计值\\(\\hat \\beta_0\\)与\\(\\hat \\beta_1\\)，代入\\(X\\)，得到模型估计值\\(\\hat Y\\) 残差平方和：\\(RSS = e_1^2 + e_2^2 + ... + e_n^2\\)，使RSS最小，求导可得参数 回归线不等于最小二乘线，最小二乘线是通过采样对回归线的估计 估计会存在偏差，均值的偏差用标准误来描述\\(Var(\\hat \\mu) = SE(\\mu)^2 = \\frac{\\sigma^2}{n}\\) 回归参数的估计也涉及标准误的计算\\(Var(\\beta_{1}) = \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\) \\(\\sigma^2\\)可用残差标准误RSE(\\(RSE = RSS/(n − 2)\\))来估计\\(\\qquad\\hat\\sigma^2 = \\frac{n-p}{n}\\;s^2\\) 据此可得回归参数的95%置信区间\\(\\hat \\beta_1 ± 2 \\cdot SE(\\hat \\beta_1)\\) 参数的评价可通过假设检验进行，零假设为\\(\\beta_1\\)为0，也就是自变量对因变量无影响，构建t统计量\\(t = \\frac{\\hat \\beta_1 - 0}{\\hat {SE}(\\hat \\beta_1)}\\)，然后可根据p值判断参数的显著性 评价参数后需要评价模型，主要通过\\(RSE\\)与\\(R^2\\)来进行 \\(R^2\\)表示模型所解释总体方差的比例，与\\(RSE\\)不同，独立于Y，\\(R^2 = \\frac{TSS - RSS}{TSS}\\) \\(R^2\\)与两变量间的相关系数是一致的，但\\(R^2\\)统计量的应用面要广于相关系数 相关系数也可进行假设检验进而判断相关的显著性 11.3.2 多元线性回归 通过统计量F检验确定回归是否显著，零假设为所有自变量系数为0 \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) 变量选择：向前选择（从0个到p个，显著则包含），向后选择（从p个到0个，不显著则剔除），混合选择（通过p的阈值调节） 因为RSS会减少，\\(R^2\\)会伴随自变量数目的增加而增加 \\(RSE\\)在多元线性线性回归中为\\(RSE = RSS/(n − p - 1)\\)，伴随自变量个数增加影响超过\\(RSS\\)减少的影响，\\(RSE\\)会增大 自变量间的影响会导致相比单一变量预测更容易出现不显著，这说明自变量间有可能可相互解释 预测的置信区间与预测区间，前者指模型的变动范围，后者指某个预测值的变动范围，考虑真值本身的变动，后者大于前者 因子变量通过对每个水平添加系数0，1来回归，也可根据需要赋值 11.3.3 线性模型延拓 线性模型基本假设：可加性与线性 去掉可加性：考虑交互作用 层级原理：交互作用项显著而主作用不显著时不可去掉主作用项 去掉线性：多项式回归 11.3.4 常见问题 关系非线性：残差图判断 误差项共相关：误差项的相关会导致标准误估计偏低，低估参数的区间使不显著差异变得显著，考虑时间序列数据，观察误差项轨迹判断 误差项方差非常数：喇叭状残差图，通过对因变量进行对数或开方来收敛方差，或者用加权最小二乘 异常值：通过标准化残差图判断 杠杆点：加入后会影响模型拟合，通过杠杆统计量判断： \\(h_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i&#39; = 1}^{n} (x_i&#39; - \\bar x)^2}\\) 多元回归中该统计量均值为\\((p+1)/n\\)，超过很多则可能为杠杆点 在标准残差-杠杆值图中，右上或右下方为危险值，左方数值对回归影响不大 共线性：共线性的变量相互可替代，取值范围扩大，标准误加大，对因变量影响相互抵消，降低参数假设检验的功效 多重共线性：引入方差膨胀因子，自变量引入全模型与单一模型方差的比值，超过5或10说明存在共相关，\\(VIF(\\hat \\beta_j) = \\frac{1}{1 - R^2_{X_j|X_{-j}}}\\) 解决共线性：丢弃变量或合并变量 共线性不同于交互作用 11.3.5 线性回归与k临近算法比较 k临近算法：\\(\\hat f(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i\\) 核心是选择k KNN算法在解决非线性问题上有优势，但一样的面对高维诅咒 线性回归可给出可解释的形式与简单的描述 11.4 分类 11.4.1 logistic回归 因变量以概率形式出现 \\(p(X) = \\frac {e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\) 变形后\\(\\frac {p(X)}{1 - p(X)}\\) 为胜率，比概率应用更实际些，去对数后为对数胜率（logit） 因变量\\(p(X)\\)与自变量间关系非线性 用极大似然估计确定参数，似然函数为\\(l(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1} p(x_i)\\prod_{i&#39;:y_{i&#39;} = 0} (1 - p(x_{i&#39;}))\\)，该函数取最大值 线性回归中，最小二乘法为极大似然估计的特例 混杂因素的解释上要考虑单因素回归与多元回归 多响应logistic回归一般被判别分析取代 11.4.2 线性判别分析 使用原因：分类离散时logistic回归不稳定，n小X正态时更稳定，适用于多响应 贝页斯理论：\\(Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l = 1}^K \\pi_lf_l(x)}\\) 其中\\(\\pi\\) 代表先验概率，估计\\(f_k(X)\\)需要对\\(x\\)的分布作出假设 自变量为1时，假定\\(f_k(x)\\)分布为正态的，有\\(f_k(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_k} exp(- \\frac{1}{2 \\sigma_k^2} (x - \\mu_k)^2)\\)，代入可得\\(p_k(x)\\)，取对数有\\(\\sigma_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k)\\)，使\\(\\sigma_k(x)\\)最大的分类方法为判定边界 贝页斯分类器需要知道所有分布参数，实际中会采用线性判别分析（LDA），通过以下训练集估计方法来插入贝页斯分类器：\\(\\hat \\pi_k = n_k/n\\)、\\(\\hat \\mu_k = \\frac{1}{n_k} \\sum_{i:y_i = k} x_i\\) 与 \\(\\hat \\sigma^2 = \\frac{1}{n - K} \\sum_{k = 1}^K \\sum_{i:y_i = k} (x_i - \\hat \\mu_k)^2\\) 线性体现在判别函数\\(\\hat \\sigma_k(x)\\)的形式是线性的 自变量多于1时，假设自变量均来自多元正态分布的分类 列连表，表示假阳性，假阴性，可计算灵敏度与特异性 LDA是对贝页斯分类的模拟，旨在降低总错误率，因此灵敏度与特异性区分并不明显，可根据实际需要调节 ROC曲线用来展示两种错误，横坐标假阳性，纵坐标真阳性 11.4.3 二次判别分析（QDA）及其它 不同于LDA，二次判别分析考虑各分类参数中方差不同而不是相同，引入了二次项 对分类描述更为精细，但容易过拟合，样本较少，LDA优先 对比logistic回归，两者数学形式相近，取值上logistic回归使用极大似然法，LDA使用共方差的高斯分布假设，结论多数条件一致，但随假设不同而不同 KNN更适用于非线性关系，标准化很有必要，QDA相对温和 11.5 重采样技术 11.5.1 交叉检验 核心思想：通过保留一部份训练集数据作为检验集来估计真实检验集的错误率与模型拟合效果 验证集方法：将训练集数据分为两部分，一部份拟合模型，一部份检验模型，这样得到的错误率为真实检验集的一个估计，选取错误率较低的模型建模 验证集方法缺点：错误率依赖于采样变动较大，训练集少，低估了错误率 留一法(LOOCV)：每次建模留一个数据点作为验证集，\\(MSE_i = (y_i - \\hat y_i)^2\\)重复n次，得到一个CV值作为对错误率的估计:\\(CV_{(n)} = \\frac{1}{n} \\sum_{i = 1}^{n} MSE_i\\) 留一法优点：使用数据量大，偏差小；结果唯一，不受随机化影响 留一法缺点：计算量大，公式插入杠杆统计量调节杠杆点对方程拟合的影响，得到\\(CV_{(n)} = \\frac{1}{n} \\sum_{i = 1}^{n} (\\frac{y_i - \\hat y_i}{1 - h_i})^2\\) k叠交叉检验：将训练集分为k叠，每次建模用(k-1)叠，用1叠检验 k叠交叉检验优点：计算量小，结果与留一法相差不多- 交叉检验的结果用来寻找\\(CV\\)值最小的点来选择模型，通常与真实检验集最小点结果相差不大乎，但交叉检验给出的\\(MSE\\)会偏低 偏差方差权衡：使用的训练集数据越多，估计偏差越小，方差越大（相关性越高的方差越大） 分类问题使用错误率计算\\(CV\\)：\\(CV{(n)} = \\frac{1}{n} \\sum_{i = 1}^{n} Err_{i}\\) 少n多p问题上使用交叉检验，不可先进行全模型变量选择再交叉检验，应该对整个过程交叉检验 11.5.2 bootstrap 在训练集里有放回的重采样等长的数据形成新的数据集并计算相关参数，重复n次得到对参数的估计，计算标准误 生成Bootstrap Percentile置信区间 适用于独立样本，样本间有相关如时间序列数据可采用block法分组屏蔽掉进行bootstrap 因为存在重复，使用bootstrap建立训练集与预测集会有非独立样本，造成检验集模型方差的低估，去掉重复使模型复杂，不如交叉检验对检验集误差估计的准 11.6 线性模型选择与正则化 最小二乘法（OLS）容易解释，预测性能好，但不万能 预测准确性上，当p&gt;n时，模型方差变大 模型解释上，p过多需要去除，进行模型选择 11.6.1 模型选择方法 11.6.1.1 子集选择 从p个自变量中选出与模型响应相关的进行建模 使用devianc，最大化为最优子集 最佳子集选择：p个自变量\\(p \\choose k\\) ，计算\\(RSS\\)与\\(R^2\\)，\\(RSS\\)要小，\\(R^2\\)要大，选择最佳的 步进法：p值过大，计算负担重，采用逐步改进法进行模型选择 向前步进选择：从0个自变量开始加，第k个自变量选择p-k个模型，如果\\(RSS\\)与\\(R^2\\)表现好就保留，递近选择变量，不保证选择最佳模型，p值较大优先考虑 向后步进选择：从p个自变量开始减，如果第k个自变量在模型\\(RSS\\)与\\(R^2\\)中没表现，就剔除进行变量选择，不保证选择最佳模型，适用于p值较小的情况(较大可能无法拟合) 步进选择构建\\(1 + p(p+1)/2\\)个模型，最佳子集法需要构建\\(2^p\\)个模型 混合模型:向前选择，之后向后验证，剔除不再提高效果的模型 11.6.1.2 测试集误差估计 \\(RSS\\)与\\(R^2\\)评价的是训练集拟合状况，不适用于估计测试集误差 估计测试集误差可以构建统计量调节训练集误差或直接通过验证集来估计 Mallow’s \\(C_p\\)：\\(C_p = \\frac{1}{n} (RSS + 2d\\hat \\sigma^2)\\) \\(d\\)代表使用的变量数，\\(\\hat \\sigma^2\\)是对模型方差的估计 AIC：\\(AIC = -2logL + 2 \\cdot d\\) 极大似然估计，线性模型下\\(C_p\\)与AIC实质等同 BIC：\\(BIC = \\frac{1}{n}(RSS + log(n)d\\hat \\sigma^2)\\) n是样本数，大于7时BIC会比\\(C_p\\)选择更轻量的模型 调节\\(R^2\\)：\\(Adjusted\\) \\(R^2 = 1 - \\frac{RSS/n-d-1}{TSS/(n - 1)}\\) 值越大，测试集误差越小 不同于\\(C_p\\)，AIC，BIC有严格的统计学意义，调节\\(R^2\\)虽然直观，但理论基础相对薄弱，单纯考虑了对无关变量的惩罚 验证与交叉验证：直接估计测试集误差而不用估计模型方差 单标准误原则：先计算不同规模测试集\\(MSE\\)的标准差，选择曲线中最小测试集误差一个标准误内最简单的模型 11.6.1.3 收缩 对系数估计进行收缩，接近0或等于0进行变量选择 11.6.1.3.1 岭回归 不同于最小二乘估计对\\(RSS\\)的最小化，岭回归最小化\\(RSS + \\lambda \\sum_{j = 1}^{p} \\beta_j^2\\)，其中\\(\\lambda\\)为调谐参数，后面一项为收缩惩罚，是个\\(l_2\\)范数，使参数估计逼近0，选择合适\\(\\lambda\\)很重要，可用交叉检验来实现 因为范数大小影响模型惩罚项，所以进行岭回归前要做标准化处理\\[\\bar x_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i = 1}^n (x_{ij} - \\bar x_j)^2}}\\] 岭回归的参数\\(\\lambda\\)与范数收缩状况可看作最小\\(MSE\\)的函数来表现偏差-误差均衡 岭回归适用于最小二乘回归产生方差较大的情况，同时，计算负担较小，只伴随\\(\\lambda\\)取值范围变化而变化 11.6.1.3.2 Lasso 形式与岭回归一致，最小化\\(RSS + \\lambda \\sum_{j = 1}^{p} |\\beta_j|\\)，使用\\(l_1\\)范数 岭回归参数同步收缩接近0，Lasso可以通过软边界直接收缩到0实现变量选择，产生稀疏模型，想像超球体与超多面体与超球面的接触 贝页斯视角下，岭回归与lasso关于线性模型系数的先验分布是不同的：前者为高斯分布，接近0时平坦，后验概率等同最优解；后者为拉普拉斯分布，接近0时尖锐，先验概率系数接近0，后验概率不一定为稀疏向量 岭回归与Lasso分别适用于真实模型自变量多或少的情况，并不广谱，考虑交叉检验来进行选择 交叉检验也可用来选择\\(\\lambda\\), 通过选择的自变量参与建模 11.6.2 降维 前提是自变量间不独立，将p个自变量向量投影到M维空间(M &lt; p)，使用投影M拟合线性回归模型 \\(\\sum_{m = 1}^{M}\\theta_m z_{im} = \\sum_{m = 1}^{M} \\theta_m \\sum_{j = 1}^{p} \\phi_{jm}x_{ij} = \\sum_{j = 1}^p \\sum_{m = 1}^{M} \\theta_m \\phi_{jm} x_{ij} = \\sum_{j = 1}^{p} \\beta_j x_{ij}\\) 主成分：各自变量在主成分方向上方差最大 主成分回归(PCA)：实际为无监督算法，得到主成分后作为新变量进行最小二乘回归，认为因变量与自变量变异最大的方向一致，需要仔细检验这个假设，主成分个数的选择影响模型效果 岭回归疑似为主成分回归的连续版，两者都需要标准化，效果也相近 偏最小二乘(PLA)：第一个投影方向为因变量与自变量回归方向，后续投影是对残差投影方向的回归，重复得到监督学习的效果 PLA通常并不比PCA更好，引入了监督算法提高了偏差 11.6.3 高维数据 n远远少于p或接近的数据 最小二乘估计在n小于p时残差为0，太过精细 \\(C_p\\)，AIC，BIC方法因为有参数\\(\\hat \\sigma^2\\)需要估计，而这个参数会在高维数据下变成0，调节\\(R^2\\)也会变成1 高维诅咒：正则化或收缩对高维方法产生影响，合适调谐参数十分重要，测试集误差必然增长 引入新变量会对预测产生不可知影响，选出的自变量并非不可替代，结果用独立验证集误差或交叉检验误差描述 11.7 非线性 11.7.1 多项式回归 模型基本形式为单一自变量在不同幂指数下的多项式，最小二乘拟合 模型在特定点的方差受系数方差与协方差影响，幂越高，模型越精细，方差越大 幂次一般不超过3或4 可进行logistic回归 11.7.2 阶梯函数 阶梯函数将自变量由连续变成有序分类变量 函数形式为引入指标函数\\(C_K(x)\\)进行自变量分段，然后进行最小二乘拟合 依赖找间隔点 可进行logistic回归 11.7.3 基函数 固定线性系数\\(\\beta\\),自变量的形式由\\(b(x)\\)决定，\\(b(x)\\)为基函数 多项式回归与阶梯函数均为基函数的特例 11.7.4 回归样条 设定分段点，分段点前后进行多项式回归 K个点分割(K+1)段，存在(K+1)个多项式回归，自由度过高 进行边界约束，对n次方程而言，约束分段点0阶，1阶，2阶导数连续，减少3个自由度，共有K个点，则有\\((n+1-3)k + n + 1\\)个自由度，相比无约束的\\((n+1)k\\)，自由度减少，更稳健 一般而言约束限制为(自由度-1)阶连续，这样自由度比分界点略多些，够用 分段样条最好在两端加入线性限制，收敛自由度，这样在边界稳健，为自然样条 分段点位置一般均匀分布，个数（本质上是自由度）通过交叉检验来确定 分段多项式回归限定了自由度，因此结果一般比多项式回归更稳定 11.7.5 平滑样条 如果以RSS衡量不加入限制，很容易产生过拟合，因此考虑加入平滑项 最小化 \\[ \\sum_{i = 1}^n (y_{i} - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2 dt \\] 其中，\\(g(x)\\)为平滑样条，由损失函数与惩罚项组成，二次导数表示在t处的平坦度，越平坦，惩罚越小，越崎岖，惩罚越大，因而平滑 对三次函数而言，平滑样条会将函数两端收敛的跟自然样条一样，实际上，平滑样条是自然样条的收缩版 参数\\(\\lambda\\)也影响平滑效果，越大越平滑，因为k固定，只涉及\\(\\lambda\\)的选择 参数\\(\\lambda\\)的选择基于有效自由度，可以用留一法进行估计，形式与杠杆点统计量差不多，可以很方便的进行数值求解 平滑样条的自由度比多项式要小，更稳健 11.7.6 本地回归 首先分段，然后分段内进行加权回归，离某点越近，权重越高，进行最小二乘拟合，得到每个点的函数，联合模型拟合 自变量较多，可考虑本地有选择的选取自变量进行本地回归 同样遭受高维诅咒带来的临近值少或稀疏问题 11.7.7 广义加性模型 \\[y_i = \\beta_0 + \\sum_{i = 1}^n f_j(x_{ij}) + \\epsilon\\] 每个自变量都有自己的函数形式，加合求解 每个自变量影响都可以展示 可分段，也可使用平滑，平滑方法中使用了反馈拟合策略对不易用最小二乘拟合求解的问题进行求解，效果差不多，分段不必要 可用于分类回归问题，解释性好 优点：非线性，更准确，易解释，可进行统计推断，可用自由度衡量平滑性 缺点：不易考虑交互影响 11.8 决策树 11.8.1 回归树 将因变量按自变量分区间，每个区间内预测值一致，直观易解释 \\(\\sum_{j = 1}^J\\sum_{i \\in R_J}(y_i - \\hat y_{R_j})^2\\) 计算困难，使用自上而下的贪心算法 递归二元分割：构建树过程每个节点都选最佳分割点，也就是分割后残差最小的变量与数值 算法在叶样本数为5时结束 树修剪，选择训练集误差最小的子树，引入调谐因子\\(\\alpha\\) 最小化\\(\\sum_{m = 1}^{|T|} \\sum_{i:x_i \\in R_m} (y_i - \\hat y_{R_m})^2 + \\alpha|T|\\) 类似lasso算法 确定，之后选出特定模型 11.8.2 分类树 因变量为分类变量，RSS用分类错误率代替 \\(E = 1 - max_k(\\hat p_{mk})\\)但分类错误率对树生长并不敏感，应采用其他指标 Gini系数：\\(G = \\sum_{k = 1}^K\\hat p_{mk}(1 - \\hat p_{mk})\\)，分类越准，值越小，衡量端纯度 cross-entropy：\\(D = - \\sum_{k = 1}^K\\hat p_{mk} log\\hat p_{mk}\\)，与Gini系数相似，描述一致 如果以修剪树为目标，指标应选择分类错误率 节点产生相同预测说明预测纯度不同，可靠性不同 与线性模型相比，适用数据种类不同，借助可视化判断 优点：容易解释，适用于决策，容易出图，处理分类问题简单 缺点：预测准确率低于其他常见回归与分类方法 11.8.3 Bagging 决策树方法相比线性回归模型方差很大 引入Bootstrap，通过平均构建低方差模型 \\(\\hat f_{avg}(x) = \\frac{1}{B} \\sum_{b = 1}^B \\hat f^{*b}(x)\\) 不修剪，通过平均降低方差 对于分类变量，通过投票，少数服从多数得到答案 误差估计通过包外样本（OOB）进行交叉检验并进行树的选择，降低计算成本 变量权重在Bagging中不易衡量，可通过衡量每棵树的RSS或者Gini系数在进行一次变量分割后RSS下降程度并进行排序取得 该方法可应用于其他统计模型 11.8.4 随机森林 bagging中使用所有的变量进行选择，但是会更易出现共相关变量，方差降低不多 随机森林的核心在于强制使用较少的自变量，为其他自变量提供预测空间进而提高模型表现 变量数一般选择为\\(\\sqrt p\\) 表现会比bagging好一些 11.8.5 Boosting 通用的统计学习方法 树生长基于先前的树，不使用bootstrap，使用修改过的原始数据 先生成有d个节点的树，之后通过加入收缩的新树来拟合残差，收缩因子为\\(\\lambda\\)，呈现层级模式，最后模型为\\(\\hat f(x) = \\sum_{b = 1}^B \\lambda \\hat f^b(x)\\) boosting学习缓慢，一般学习较慢的学习效果更好 三个参数：树个数B（交叉检验），收缩因子\\(\\lambda\\)（控制学习速率），树节点数（一般为1，为交互作用深度，控制涉及变量） 深度d为1时是加性模型 随机森林与Boosting产生的模型都不好解释 11.9 支持向量机 11.9.1 最大边界分类器 11.9.1.1 超平面 p维空间里(p-1)维子空间 \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p = 0\\) 定义一个p维超平面，X落在超平面上 p维空间中点X不在超平面上就在其两侧 11.9.1.2 超平面分类 n*p矩阵X分为两类Y-1或1 代入超平面大于0为1，小于0为-1，有\\(Y*\\beta*X &gt; 0\\) 表示分类正确 构建训练函数\\(f(x^*) = \\beta_0 + \\beta_1 X_1^* + \\beta_2 X_2^* + ... + \\beta_p X_p^*\\) 正数表示为1，负数为-1，距离0越远表示距离超平面越远，越近表示分类越不确定，判定边界为线性 11.9.1.3 最大边界分类器 最大边界超平面：距离边界最近的距离的所有超平面中距离边界点最远的那个超平面 分类良好但容易在p大时过拟合 形成最大边界分类器所需要的边界点为支持向量，用以支持最大边界超平面 \\(f(x^*)*y_i\\)在系数平方和为1时为点到平面的垂直距离，最小化后最大化这个距离是求最大边界超平面的关键 11.9.2 支持向量分类器 有些情况不存在超平面，需要求一个软边界来适配最多的分类，这就是支持向量分类器 因为是软边界所以允许在超平面或边界一边出现误判 计算上还是为最小化最大化距离，但分类上距离要乘以\\(1 - \\epsilon_i\\)项，也就是松弛变量 松弛变量大于0表示边界误判，大于1表示超平面误判，总和为C，表示边界的容忍度，越大分类越模糊 C可通过交叉检验获得，控制bias-variance权衡 只有边界内观察点影响超平面的选择，这些点为支持向量，是形成模型的关键 与LDA不同，使用部分数据，与logistic回归类似 11.9.3 支持向量机 非线性条件下可以考虑将超平面理解为非线性超平面，提高样本维度换取分类效果 加入多项式等非线性描述后计算量不可控 支持向量机通过核来控制非线性边界 通过样本内积来解决支持向量分类问题 线性支持向量分类器\\(f(x) = \\beta_0 + \\sum_{i = 1}^{n} \\alpha_i &lt; x,x_i &gt;\\) 只有支持向量在解中非0，现在只需要支持向量的内积就可以求解 内积可以推广为核函数，核函数可以采用非线性模式 \\(f(x) = \\beta_0 + \\sum_{i = 1}^{n} \\alpha_i K( x,x_i )\\) 径向基核函数较为常用 使用内积的核函数计算上简单且等价与高维空间超平面分类 11.9.3.1 多于二分类 一对一分类：对比\\(K \\choose 2\\)个分类器在检验集中的效果，通过计数来选择分类结果 一对多分类：对比K个与剩下的K-1个分类，分类结果最远的认为属于那个分类 11.9.4 与logistic回归关系 中枢损失，对关键点敏感 传统方法也可以借鉴核函数观点视同 支持向量无法提供参数概率信息，采用核函数的logistic回归可以，计算量大 分类距离较远，支持向量机会比logistic回归好一点 支持向量机是计算机背景，logistic回归是概率背景 11.10 无监督学习 11.10.1 主成分分析 用较少的变量代表较多的变量，方便可视化与理解数据 第一主成分\\(Z_1 = \\phi_{11} X_1 + \\phi_{21}X_2 + ... + \\phi_{p1} X_p\\) 方差最大， 正则化后有\\(\\sum_{j = 1}^p \\phi_{j1}^2 = 1\\)，则\\(\\phi\\)为变量在第一主成分上的载荷 求解上第一主成分最大化\\(\\frac{1}{n} \\sum_{i = 1}^{n} z_{i1}^2\\) 求解载荷值，\\(z_{ni}\\)是第一个样本在第一个主成分上的得分 载荷表示变量重要程度，得分表示样本重要程度 第二主成分与第一主成分正交求解 biplot 同时表示载荷与得分，载荷向量接近表示有相关性，方向不一表示相关性弱，变量在主成分得分差异表示其状态 第一个主成分表示在p维空间里距离n个观察最近的超平面，因此具备代表性 取M个主成分可代表所有数据\\(x_{ij} \\approx \\sum_{m = 1}^M z_{im} \\phi_{jm}\\) 变量单位要统一，已经统一就不要标准化了 主成分是唯一的，符号可能有变化，载荷与得分值也唯一 主成分的重要性通过方差解释比例(PVE)来衡量，用碎石图来可视化\\[\\frac{\\sum_{i = 1}^n (\\sum_{j =1}^p \\phi_{jm} x_{ij})^2}{\\sum_{j =1}^p x_{ij}^2}\\] 寻找碎石图的肘部来确定选取主成分的个数，方法不固定 可用来进行M小于p的主成分回归 11.10.2 聚类方法 寻找子分类或簇的方法，从异质性寻找同质性 11.10.2.1 k均值聚类 子类中方差小，子类间方差大 事先指定子类个数 最小化所有K个平均欧式距离\\(W(C_k) = \\frac{1}{|C_k|} \\sum_{i,i&#39; \\in C_k} \\sum_{j = 1}^{p} (x_{ij} - x_{i&#39;j})^2\\) 先对所有样本随机分类，然后每种分类取中心，选取里中心距离最近的点重新分类，重新计算中心，迭代得到聚类结果 11.10.2.2 分层聚类 不需要指定先前聚类数，形成冰柱图 冰柱图要垂直分层解释，水平解释容易出现误导- 修剪冰柱图可给出聚类数 计算所有样本间距离，越相近就融合为一类，重新计算距离，反复这一过程 计算两者间相似度是很关键的，不同场景应用不同算法 变量的标准化处理上也很重要，考虑实际场景 "],
["section-12.html", "Notes 12 基因组学数据分析 12.1 microarrays 12.2 NGS 12.3 数据分析应用背景 12.4 Bioconductor 12.5 示例：甲基化数据分析", " Notes 12 基因组学数据分析 主页 12.1 microarrays 12.1.1 原理 生成互补DNA探针 标记样品中的DNA单链 可以对不同样品标记不同颜色 特异性互补反应 测定标记物光信号 12.1.2 应用 测定基因表达 已知序列 3’端（降解从5’端开始）选取11个片段作为探针 样品对11个片段都是高表达则基因高表达 寻找SNP SNP 单核苷酸多态性 用来探索基因型 合成SNP探针 测定对不同探针的响应判断AA AG GG类型 寻找转录因子结合位点 样品处理为含蛋白与不含蛋白两份 去除蛋白后扩增 探针是基因组感兴趣的片段 瓦片分析可知探针与含转录因子DNA结合位点 总DNA作为对照 12.2 NGS 12.2.1 原理 DNA打成50~70片段 一个样品片段上亿 加上adaptor 固定在板上后原位扩增成束 使用标记过的单核苷酸逐碱基对测光强 同时测序大量片段 得到测序结果与强度 12.2.2 应用 寻找SNP RNA-seq 测定RNA表达量 寻找结合位点 表达量 12.3 数据分析应用背景 12.3.1 DNA 甲基化 CpG 5’端到3‘端 CG C上甲基化 复制时该特性会保留 临近CpG位点的基因不会被表达 CpG 成簇存在 称为CpG islands bisulfite treatment 可以用来测定CpG是否被甲基化 通过将未甲基化的CpG中的C改为T 测序中测定改变率就可知CpG位点甲基化程度与位置 12.3.2 CHIP-SEQ 蛋白结合后固定，洗掉其余片段，然后洗掉蛋白，对序列片段测序得到结合位点 12.3.3 RNA 测序 RNA反转录为cDNA测序 只有外显子 同一基因多种RNA片段 均值与方差有相关性 需要进行log变换后分析 12.4 Bioconductor 官方说明 使用biocLite()安装，安装后仍需要library()才能使用 source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite() 12.4.1 数据结构 12.4.1.1 分析数据 F 行 S 列 F代表芯片特征数，S代表样本数 12.4.1.2 表型数据 S 行 V 列 V代表样本特征，为分类或连续变量 如果表型数据解释不清，可以建立一个解释样本特征的labelDescription数据框，通过phenoData &lt;- new(&quot;AnnotatedDataFrame&quot;,data=pData, varMetadata=metadata) 建立AnnotatedDataFrame类型数据 12.4.1.3 实验描述 MIAME 类型对象 描述实验参数 12.4.1.4 组装数据 将分析数据，表型数据，实验描述组装为一个ExpressionSet类型的对象 exampleSet &lt;- ExpressionSet(assayData=exprs,phenoData=phenoData,experimentData=experimentData,annotation=&quot;hgu95av2&quot;) annotation代表了一组相似实验设计的芯片数据的代号，通过相关代号可以索引到芯片特征信息并将其与其他数据如基因型，染色体位置等连接以便于分析 从ExpressionSet里可以按照表型数据提取子集，也就是对 S 截取 V 中特定子集 exampleSet[ , exampleSet$gender == &quot;Male&quot;] esApply 用来针对ExpressionSet应用函数 12.4.1.5 数据集应用 library(Biobase) library(GEOquery) geoq &lt;- getGEO(&quot;GSE9514&quot;) 从基因表达精选集（GEO）上得到数据表达集 names(geoq) 得到文件名 e &lt;- geoq[[1]] 得到数据集 dim(e) 查看表达集维度 给出样本数与特征值，也就是测定序列数 dim(exprs(e)) 与上面等同，给出基因分析数据 dim(pData(e)) 给出8个样本的信息，信息头用names(pData(e))给出 dim(fData(e)) 给出特征与信息头列表 exprs为特征数×样本数矩阵 pdata为样本数×信息头 fdata为特征数×信息 experimentData(e) 给出实验信息 annotation(e) 特征注释 exptData(se)$MIAME 给出实验相关关键信息 Y &lt;- log2(exprs(bottomly.eset) + 0.5) 对NGS数据加0.5取2为底的对数（防0）得-1，排除掉0后可得MAplot观察数值分布，一般为均值小差异大，均值大相对稳定 formula 用来定义公式 model.matrix 用定义的公式生成矩阵 rowttests(y[, smallset], group[smallset]) 定义分组，设定模型可进行t-test，用火山图来表示 12.4.1.5.1 Iranges library(IRanges) 序列范围 ir &lt;- IRanges(start = c(3, 5, 17), end = c(10, 8, 20)) 定义序列 IRanges(5, 10) 表示5到10这6个碱基对，可以shift range(ir) 表示存在ir中序列的起止范围 gaps(ir) 表示寻找ir中间隔片段 disjoin(ir) 表示将ir中序列碎片化后互不重叠的片段 12.4.1.5.2 GRanges and GRangesList library(GenomicRanges) 基因范围 gr &lt;- GRanges(&quot;chrZ&quot;, IRanges(start = c(5, 10), end = c(35, 45)), strand = &quot;+&quot;, seqlengths = c(chrZ = 100L)) 定义位于染色体chrZ上几个序列范围，认为这些范围共同定义一个基因 可以shift，可以定义长度后trim mcols(gr)$value &lt;- c(-1, 4) 定义该基因类型中的列并赋值 grl &lt;- GRangesList(gr, gr2) 多个Granges定义一个基因库 length(grl) 给出基因库里基因个数 mcols(grl)$value &lt;- c(5, 7) 定义该基因库类型中的列并赋值 12.4.1.5.3 findOverlaps gr1 gr2 为两个基因范围对象 fo &lt;- findOverlaps(gr1, gr2) 寻找两个基因重叠序列 queryHits(fo) 与 subjectHits(fo) 提取两个基因重叠序号 成对出现 gr1[gr1 %over% gr2] 提取对应序列范围 12.4.1.5.4 Rle Rle(c(1, 1, 1, 0, 0, -2, -2, -2, rep(-1, 20))) 表示4组处理，每组各有 3 2 3 20 个重复 Rle是一种压缩存储实验设计的方式，可以用as.numeric()提取原始数据 Views(r, start = c(4, 2), end = c(7, 6) 提取对应实验组 12.4.2 数据读取 microarray 或 NGS 数据由芯片厂商提供，常见读取原始信息的包有affyPLM、affy、 oligo、limma 在Bioconductor里，这些原始数据要转为ExpressionSet格式 12.4.2.1 Affymterix CEL files library(affy) tab &lt;- read.delim(&quot;sampleinfo.txt&quot;, check.names = FALSE, as.is = TRUE) 读取样本信息 ab &lt;- ReadAffy(phenoData = tab) 读取样本数据，探针层次 ejust &lt;- justRMA(filenames = tab[, 1], phenoData = tab) 直接读取为基因层数据 e &lt;- rma(ab) 对样本进行背景校正与正则化，从探针层转化为基因层数据 12.4.2.2 背景干扰 spikein方法 梯度加入已知浓度的基因片段 阵列上进行shift 类似拉丁方设计 可以看到同一基因不同片段大致符合先平后增模式 开始阶段是噪声主导 后面是浓度主导 使用类似基因模拟噪声主导 相减后得到去干扰浓度效应 但低值部分会导致方差过大 也可以使用统计建模方法模拟背景值与响应 得到还原度更高的信号 12.4.2.3 正则化 基因组数据大多数为0 加标样品变化 正则化是为了还原这一结果 分位数正则化 局部回归正则化 稳方差正则化 当重复实验时 直接用分位数正则会掩盖样品差异 可以考虑只对加标基因正则化 然后推广到全局 12.4.2.4 探索分析作图 12.4.2.4.1 MA-plot x轴为两组基因组的均值，y轴为两组基因组的均值差 用来表示两组平行间的差异 12.4.2.4.2 Volcano plot 横坐标为处理间基因表达差异，纵坐标为差异的-log10(p.value) 一般为火山喷发状，差异越大，p值越小 12.5 示例：甲基化数据分析 12.5.1 读取数据 devtools::install_github(&quot;coloncancermeth&quot;,&quot;genomicsclass&quot;) library(coloncancermeth) data(coloncancermeth) 该数据集为结肠癌病人与对照的DNA甲基化数据集。 12.5.2 数据说明 dim(meth) dim(pd) length(gr) meth为测序数据，pd为样本信息，gr测序片段信息。 colnames(pd) table(pd$Status) X = model.matrix(~pd$Status) 查看病患与正常人的分组并构建模型。 chr = as.factor(seqnames(gr)) pos = start(gr) library(bumphunter) cl = clusterMaker(chr,pos,maxGap=500) res = bumphunter(meth,X,chr=chr,pos=pos,cluster=cl,cutoff=0.1,B=0) 按染色体生成因子变量，找出基因起始位点，然后利用bumphunter包寻找甲基化数据中某个阈值（0.1）下甲基化基因聚类的后出现的位置，聚类号，聚类相关性等信息寻找问题基因，可从中提取相关信息 cols=ifelse(pd$Status==&quot;normal&quot;,1,2) Index=(res$table[6,7]-3):(res$table[6,8]+3) matplot(pos[Index],meth[Index,,drop=TRUE],col=cols,pch=1,xlab=&quot;genomic location&quot;,ylab=&quot;Methylation&quot;,ylim=c(0,1)) Index=(res$table[6,7]):(res$table[6,8]) test &lt;- meth[Index,,drop=T] colnames(test) &lt;- pd$bcr_patient_barcode test1 &lt;- test[,cols==1] test2 &lt;- test[,cols==2] test3 &lt;- apply(test2, 2, mean) apply(matrix, 1, rank) 从上面可以得到有差异的甲基化数据所在的基因位置并提取相关样本数据信息。可根据差异作图，得到两组数据甲基化水平差异所在的基因位置。可对差异进行平滑操作，得到位置。这样就可以知道甲基化发生的序列位置与水平差异的信息了。 下面的例子是用人类基因组数据探索潜在的CpG岛。 library(BSgenome.Hsapiens.UCSC.hg19) Hsapiens[[&quot;chr1&quot;]] # 计算某染色体上潜在位点个数 countPattern(&#39;CG&#39;,Hsapiens[[&quot;chr1&quot;]]) # 计算某染色体上特定序列比例 观察与期望出现的比例 CG &lt;- countPattern(&#39;CG&#39;,Hsapiens[[&quot;chr1&quot;]])/length(Hsapiens[[&quot;chr1&quot;]]) GC &lt;- countPattern(&#39;GC&#39;,Hsapiens[[&quot;chr1&quot;]])/length(Hsapiens[[&quot;chr1&quot;]]) table &lt;- alphabetFrequency(Hsapiens[[&quot;chr1&quot;]]) expect &lt;- table[&#39;C&#39;]%*%table[&#39;G&#39;]/(length(Hsapiens[[&quot;chr1&quot;]]))^2 CG/expect "]
]
